[
    {
        "unidad":"Preparación de datos",
        "titulo":"Normalización sobre dataset Wine en RapidMiner",
        "descripcion":"En RapidMiner se realizó una normalización sobre el dataset Wine y se comparó la performance de aplicar Naive Bayes usando el dataset normalizado y no normalizado con un split con el 70% de ejemplos para training y 30% para test en ambos casos.",
        "contenido":"h3>>Pasos=,=p>>- Se creó un proceso en RapidMiner con dos canales, uno que utiliza el dataset normalizado con transforamción Z y uno que no.=,=p>>- En ambos canales se realizó un split con 70% de datos para entrenamiento y 30% para test. =,=p>>- Para cada caso se entrenó un modelo de Naive Bayes para clasificación por lo cual hubo que convertir el atriburto Wine de numérico a polinómico.=,=p>>- Se le midió performance del modelo.=,=br=,=h3>>Resultados=,=img>>UT2-PD1-3.jpg>>100%>>400=,=br=,=a>>Descargar modelo>>UT2 PD1 ej2.rmp>>d=,=br=,=br=,=p>>Caso normalizado=,=img>>UT2-PD1-4.jpg>>100%>>200=,=p>>Caso sin normalizar=,=img>>UT2-PD1-5.jpg>>100%>>200=,=br=,=p>>Se obtuvo un poco de mayor presición para el caso del dataset normalizado lo cual se debe al algoritmo utilizado. Naive Bayes calcula frecuencias para hallar las clases de predicción y al estandarizar los datos, las frecuencias se mantienen de una forma similar."
    },
    {   
        "unidad":"Preparación de datos",
        "titulo":"Tutoriales de preparación de datos en RapidMiner",
        "descripcion":"En este ejercicio se hicieron tutoriales de missing values, normalización, detección de outliers, modelado, scoring, split y validación, cross validation y visual model comparison para la herramienta RapidMiner.",
        "contenido":"h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>100%>>200=,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>100%>>150=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h3>> Modeling =,=img>>UT2-PD2-1.jpg>>100%>>300 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>100%>>250 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>100%>>175 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>100%>>200=,=br=,=br=,=img>>UT2-PD2-4-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>100%>>125=,=br=,=br=,=img>>UT2-PD2-5-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d"
    },
    {
        "unidad":"Preparación de datos",
        "titulo":"Pre procesamiento de datos con Python",
        "descripcion":"Para este caso se aplican técnicas de pre procesamiento como normalización, estandarización y split en datos de training y test. Luego se obtienen estadísticas para el dataset Wine utiliando funciones de Python con SciKitLearn y la librería Pandas.",
        "contenido":"h3>>Pasos=,=p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0    dataset = list()=,=br=,=code>>.\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0    minmax = list()=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0    train = list()=,=br=,=code>>.\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.\u00a0for i in range(10):    =,=br=,=code>>.\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>100%>>300=,=img>>UT2-PD3-2.jpg>>100%>>300=,=img>>UT2-PD3-3.jpg>>100%>>300=,=img>>UT2-PD3-4.jpg>>100%>>600=,=img>>UT2-PD3-5.jpg>>100%>>600"
    },
    {
        "unidad":"Preparación de datos",
        "titulo":"Data pre processing y cálculo de probabilidades de sucesos sobre el dataset Titanic con Python",
        "descripcion":"Sobre el dataset Titanic se realizó un tratamiento de missing values con reemplazo, remoción, sustitución por categoría única y selección de atributos. Posteriormente se realizaron consultas probabilísticas sobre los datos.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=br=,=p>> - Se importaron librerías y paquetes.=,=p>> - Se cargó el dataset y se los mostró.=,=p>> - Se gestionaron los datos faltantes.=,=p>> - Se graficaron los datos según Age vs Survived y Fare vs Survived..=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\"-----DATASET------\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----MISSSING VALUES------\")=,=br=,=code>>.print(\"*Antes de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Se reemplazan los missing values de edad por la media=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan las columnas body y cabin por altos porcentajes =,=br=,=code>>.# de missing values siendo estos 90.7% y 77% respectivamente=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan la fila sin valor para fare y las dos filas=,=br=,=code>>.# Sin valor para embarked=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#Se asigna una categoría única para los missing values de boay y home.dest=,=br=,=code>>.dataset[\"boat\"].fillna('U', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('U', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"*Despues de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----GRAFICAS------\")=,=br=,=code>>.print(\"- x = Age, y = Survived\")=,=br=,=code>>.print(\"- x = Fare, y = Survived\")=,=br=,=code>>.colors = (\"red\", \"blue\")=,=br=,=code>>.plt.scatter(dataset['age'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset['fare'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-1.jpg>>300>>600=,=img>>UT2-PD4-2.jpg>>400>>300=,=img>>UT2-PD4-3.jpg>>400>>300   =,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>> - Se calculó la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> - Se calculó la probabilidad de que un niño de 10 años o menos de 3ra clase sobreviva. =,=br=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilidades:\")=,=br=,=code>>.print(\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3)))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>>.print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>500>>200"
    },
    {
        "unidad":"Preparación de datos",
        "titulo":"Selección de atributos para dataset Sonar",
        "descripcion":"Ejercicio con selección de atributos en RapidMiner sobre el dataset Sonar.",
        "contenido":"p>>En la herramienta RapidMiner usaremos el dataset Sonar y compararemos los resultados de performance al aplicar un simple algoritmo de Naive Bayes como línea base, Forward Selection con Naive Bayes y Backward Elimination con Naive Bayes empleando cross validation con 5 folds, muestreo estratificado y la misma seed para los tres casos.=,=h4>>Gráficas=,=p>>Se observa una tendencia en las frecuencias capturadas por el sonar. Vemos una amplitud y varianza clara en ambas clases. El problema que se observa a primera vista, es que las frecuencias se solapan demasiado, haciendo muy difícil su diferenciación.=,=img>>UT4-PD9-1.jpg>>900>>400=,=img>>UT4-PD9-2.jpg>>900>>400=,=br=,=br=,=h4>>Resultados=,=br=,=h6>>Línea base Naive Bayes=,=img>>UT4-PD9-3.jpg>>900>>130=,=br=,=br=,=h6>>Forward Selection=,=img>>UT4-PD9-4.jpg>>900>>130=,=p>>Mediante este algoritmo de feature selection se redujeron los atributos utilizados y la precisión de predicciones para la clase mina mejoró sustancialmente, esto quiere decir que usando solo los atributos relevantes, que representan una mayor variación en la información la precisión aumentó. Los atributos elegidos fueron los 12, 15, 17 y 18.=,=h6>>Backward Elimination=,=img>>UT4-PD9-5.jpg>>900>>130=,=p>>Este algoritmo de feature selection eliminó 8 atributos y con ello se mejoró la performance respecto a la línea base al igual que en el caso anterior.=,=a>>Descargar modelo RapidMiner>>UT4-TA9.rmp>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Regresión lineal simple en hoja de cálculo",
        "descripcion":"Utilizando una planilla electrónica se generó un modelo de regresión linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se usaron los modelos para hacer predicciones.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se calcularon los coeficientes para un modelo de regresión lineal simple Y = B0 + B1 x X. Para ello se utilizarán las fórmulas:=,=img>>UT3-PD1-1.jpg>>600>>100=,=br=,=br=,=img>>UT3-PD1-2.jpg>>500>>50=,=p>> -Se aplicó el modelo a los valores de entrenamiento.=,=p>> -Se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gráfico para mostrar la recta de ajuste.=,=p>> -Se estimó el error de predicción RMSE con esta ecuación:=,=img>>UT3-PD1-3.jpg>>400>>100=,=p>> -Se utilizó un método alternativo para calcular B1 y a partir de este B0 al igual que antes. La fórmula utilizada fue la que se encuentra a continuación:=,=img>>UT3-PD1-4.jpg>>400>250=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>>-Se estimaron los coeficientes del modelo simple de regresión lineal Y = B0 + B1 x X realizando 24 iteraciones.=,=p>> -Se realizó el gráfico error de predicción contra iteraciones =,=p>> -Se calculó el error medio cuadrático RMSE.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Regresión logística en planilla electrónica",
        "descripcion":"En una planilla electrónica se visualizará una función logística y se genararán modelos de regresión logística con descenso de gradiente estocástico.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se crearon 10 valores para una variable X con distribución normal=,=p>> -Se aplicó la función logística a los anteriores con coeficientes elegidos arbitrariamente para hallar valores de una variable Y.=,=p>> -Se generó un gráfico para visualizar la función.=,=img>>UT3-PD2-1.jpg>>700>>300=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>> - A partir de un conjunto de datos se hizo una gráfica con dos series, una para Y=0 y otra para Y=1. =,=p>> - Se estimaron los coeficientes del modelo de regresión logística Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estocástico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 épocas. =,=p>> - Se calculó la predicción, el cuadrado del error, el error de clase para cada ejemplo de entrenamiento de cada época.=,=p>> - Se calculó la exactitud y el error medio cuadrático RMSE para cada época.=,=p>> - Se realizaron las gráficas exactitud vs época y RMSE vs época.=,=p>> - Se repitió todo el proceso anterior para un nuevo conjunto de datos.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"LDA en hoja de cálculo",
        "descripcion":"Empleando una planilla electrónica se generará un modelo de análisis de discriminante lineal LDA y con él se hará la predicción de clase para un conjunto de datos con un atributo y una salida con dos clases.",
        "contenido":"h3>>Pasos=,=p>> - Se insertaron los valores del dataset. =,=p>> - Se graficaron los datos separando según las clases de salida. =,=p>> - Se calcularon, siendo n el número de clases, las medias para cada clase k y la varianza de x con las ecuaciones que se encuentran a continuación.=,=img>>UT3-PD3-1.jpg>>250>>75=,=img>>UT3-PD3-2.jpg>>350>>75 =,=p>> - Se calculó la predicción de clases usando: =,=img>>UT3-PD3-4.jpg>>400>200 =,=p>> - Se utilizó la siguiente ecuación para calcular los discriminantes de x para ambas clases Y = 0 e Y = 1.=,=img>>UT3-PD3-3.jpg>>500>>60=,=p>> -Se calculó finalmente la predicción de clase para los datos comparando los discriminantes, el error de predicción y la exactitud de la predicción total del modelo. =,=h3>>Resultados=,=p>>La hoja de trabajo donde se hizo la práctica puede descargarse en este enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Regresión lineal con descenso de gradiente",
        "descripcion":"En este ejercicio se usa una planilla electrónica para minimizar una función siguiendo los gradientes de la función de costo y luego armar un modelo de regresión lineal.",
        "contenido":"h2>>Pasos=,=p>>- Se representó en la planilla un dataset con una variable de entrada X y una de salida Y.=,=p>> - Se graficaron los datos. =,=img>>UT3-PD4-1.jpg>>500>400=,=p>> - Se realizó el procedimiento de descenso de gradiente con alpha=0.01 en 24 iteraciones hallando valores para los coeficientes B0 y B1 que en una regresión lineal siguen la relación Y = B1xX + B0, la predicción para dicho modelo y el error de predicción. =,=p>> - Se graficó el error de predicción vs iteraciones. =,=img>>UT3-PD4-2.jpg>>500>400=,=p>> - Se calculó el error medio cuadrático RMSE. =,=p>> - Se generaron nuevos valores de entrada para X entre 0 y 8 con un paso 0.1 y se predijo su valor de Y a partir de un modelo de regresión lineal usando los coeficientes hallados al finalizar el procedimento mencionado anteriormente. El resultado fue graficado. =,=img>>UT3-PD4-3.jpg>>500>400 =,=p>> - Se analizaron los datos de entrada desde la óptica de los requerimientos para aplicar un método de regresión lineal.=,=br=,=h3>>Resultados=,=p>> La planilla de trabajo donde se realizaron los pasos descriptos anteriormente se encuentra accesible para la descarga con el enlace que se encuentra a continuación:=,=a>>Descargar plantilla>>Algoritmos lineales PD4.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Comparación de LDA con Python y RapidMiner para el dataset Sports",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>600>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100. =,=p>>- Se transformaron los atributos string a números. =,=p>> - Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equivalente en la herramienta RapidMiner.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>900>>300=,=br=,=a>>Descargar proceso>>Algoritmos Lineales PD5.rmp>>d=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Titanic",
        "descripcion":"Se realizará un análisis del problema y de los datos del dataset Titanic para a partir de este crear un modelo de regresión logística con la herramienta RapidMiner y otro equivalente con SciKitLearn, medir sus performances y compararlas.",
        "contenido":"h2>>Problema=,=p>>A partir de datos respecto al hundimiento de Titanic se pretende predecir si una persona con determinadas características sobrevivió o no por lo cual es un problema supervisado de clasificación binaria. Para este se propone una solución con un algoritmo lineal de regresión logística utilizando CrossValidation para el testeo de su performance.=,=h2>>Análisis de datos=,=p>>El dataset cuenta con 13 atributos de entrada más la variable objetivo \"survived\" para los cuales se cuenta con 1310 observaciones.=,=h4>>Atributos=,=p>>- pclass: Clase del pasajero. Tipo integer.=,=p>>- name. Nombre del pasajero. Tipo string.=,=p>>- sex. Sexo del viajante. Tipo string.=,=p>>- age. Edad de la persona. Tipo real. Hay 263 datos faltantes para esta variable.=,=p>>- sibsp: Número de hermanos a bordo. Tipo entero.=,=p>>- parch: Número de padres a bordo. Tipo entero.=,=p>>- ticket: Código del ticket del pasajero. Tipo string.=,=p>>- fare: Tarifa pagada. Tipo real. Hay un dato faltante para esta entrada.=,=p>>- cabin. Tipo string. Cabina del viajero. Tipo string. Tiene 1014 missing values. =,=p>>- embarked: Puerto en el que embarcó. Tipo string. Faltan dos de estos datos.=,=p>>- boat: Código del bote salvavida que usó la persona. Tipo string. Cuenta con 823 missing values.=,=p>>- body: Número de cuerpo si falleció. Tipo integer. Cuenta con 1188 missing values. =,=p>>- home.dest: Lugar de destino. Tipo string. Tiene 564 missing values.=,=p>>- survived: Si sobrevivió o no. Tipo integer.=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>Entre las entradas hay variables con alto porcentaje de datos faltantes por lo que no se contó con ellas siendo estas body con un 90.7% y cabin con un 77.4%, para la variable age se reemplazaron los valores faltantes por el promedio, se eliminaron las filas con missing values para fare y embarked y se le asignó una categoría única para los missing values de boat y home.dest. Luego de todo este proceso se eliminaron todos los missing values.=,=h5>>Outliers=,=p>>El algoritmo de regresión logística es altamente influenciable a los outliers y por lo tanto tras observar los histogramas de los datos para los distintos parámetros, que se encuentran a continuación, se eliminaron aquellos con una alguna/s de las siguientes condiciones age>=70, sibsp>=6, parch>=6 o fare>=350.=,=img>>caso-2-2.jpg>>300>>150=,=img>>caso-2-3.jpg>>300>>150=,=br=,=img>>caso-2-4.jpg>>300>>150=,=img>>caso-2-5.jpg>>300>>150=,=h5>>Correlación=,=p>>Observando la matriz de correlación se observó que hay una correlación grande entre home.dest y pclass con 0.887 y otra bastante significativa con un 0.612 entre fare y pclass de forma inversamente proporcional. Por lo tanto se decidió quitar pclass.=,=img>>caso-2-6.jpg>>600>>200=,=br=,=br=,=h5>>Selección de atributos=,=p>>Se descartaron las variables name y ticket por ser ids.=,=h3>>Modelo=,=p>>Se realizó el procedimiento de data pre-processing descripto anteriormente y luego se utilizó un algoritmo de regresión logística con CrossValidation de 10 folds y muestreo estratificado para el testeo.=,=h5>>Diseño de RapidMiner=,=img>>caso-2-7.jpg>>800>>400=,=p>>A continuación se mostrará la configuración bloque a bloque del esquema de RapidMiner y se presentará a la vez el código del modelo idéntico creado en SciKitLearn.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>400>>75.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>600>>400.=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\",\"name\",\"pclass\",\"ticket\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>Primer replace Missing Values=,=img>>caso-2-rm-3.jpg>>400>>175=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=br=,=h5>>Segundo replace Missing Values=,=img>>caso-2-rm-4.jpg>>600>>400=,=br=,=code>>.dataset[\"boat\"].fillna('0', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('0', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>600>>400.=,=br=,=code>>.dataset = dataset[dataset['sibsp'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['parch'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['age'] <= 70]=,=br=,=code>>.dataset = dataset[dataset['fare'] <= 350]=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6.jpg>>600>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\"sex\"])=,=br=,=code>>.encoded_sex_training = le.transform(dataset[\"sex\"])=,=br=,=code>>.dataset[\"sex\"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\"embarked\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"embarked\"])=,=br=,=code>>.dataset[\"embarked\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"home.dest\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"home.dest\"])=,=br=,=code>>.dataset[\"home.dest\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"boat\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"boat\"])=,=br=,=code>>.dataset[\"boat\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2-rm-7.jpg>>100%>>150.=,=br=,=code>>.X = dataset.drop(labels=[\"survived\"], axis=1)=,=br=,=code>>.y = dataset[\"survived\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>>.cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)=,=br=,=code>>.print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))=,=br=,=br=,=h4>>Resultados=,=p>>Se obtuvo un muy alto nivel de presición en RapidMiner con un 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>100%>>200=,=br=,=br=,=img>>caso-2-10.jpg>>200>>25=,=p>> En cuanto al modelo de SciKitLearn se obtuvo una presición de 94.5% +/- 1.2%. De esta forma se puede destacar que se obtuvieron presiciones similares para de los modelos idénticos de regresión logística aplicados al dataset Titanic realizados en las dos herramientas.=,=br=,=a>>Descargar modelo RapidMiner>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Titanic Logistic Regression.py>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Deportes",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h3>>Problema=,=p>>Según características personales se desea predecir cual es el mejor deporte para una persona por lo cual este es un problema supervisado de clasificación. Para este se planteará un modelado con Linear Descriminant Analysis a continuación.=,=h3>>Análisis de datos=,=p>>El dataset tiene 8 columnas de entrada numéricas y la variable objetivo categórica \"DeportePrimario\" conteniendo 493 filas.=,=h4>>Atributos=,=p>>- Edad=,=p>>- Fuerza=,=p>>- Velocidad =,=p>>- Lesiones=,=p>>- Vision =,=p>>- Resistencia =,=p>>- Agilidad =,=p>>- CapacidadDecision =,=p>>- DeportePrimario=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>En el dataset no hay datos faltantes.=,=h5>>Outliers=,=p>>Para LDA es importante quitar los outliers y tras observar los histogramas de los atributos se decidió quitar los datos con CapacidadDecision<3 o CapacidadDecision>100. La distribución para esta variable es la siguiente.=,=img>>caso-3-1.jpg>>300>>200=,=h4>>Estándarización=,=p>>El algoritmo a utilizar asume que las variables de entrada tienen la misma varianza por lo que es buena idea estandarizar los datos para que tengan media 0 y varianza 1.=,=h3>>Modelo=,=p>>El procedimiento descripto anteriormente y el modelo LDA fueron realizados de forma equivalente en RapidMiner y SciKitLearn. Se contó con un dataset de entrenamiento y uno de prueba para el cual realizar predicciones.=,=h4>>Diseño de RapidMiner=,=img>>UT3-PD5-3.jpg>>900>>300=,=br=,=h4>>Código SciKitLearn=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar proceso>>Sports LDA.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Sports LDA.py>>d=,=br=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"CART para clasificación binaria simple",
        "descripcion":"Emplearemos un modelo CART para un problema de clasificación binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se ingresó el dataset de entrada y se graficaron los datos diferenciando las clases Y = 0 e Y = 1. =,=img>>UT4-PD1-1.jpg>>600>>200=,=p>> - Se utilizó la variable X1 para armar el modelo CART con un valor para el punto de división de X1 = 2.771244718 calculando el índice Gini del modelo producido. =,=p>> - Se repitió el proceso anterior con un punto de división de X1 = 6.642287351 y se obtuvo el valor del coeficiente Gini del CART generado además de una descripción del árbol de decisión generado.=,=h2>> Ejercicio 2 =,=p>> Se utilizó el CART producido al final del ejercicio anterior para hacer predicciones de un dataset de test y se calculó la exactitud.=,=h3>>Resultados=,=p>> Las planillas de trabajo en las que se hicieron los pasos explicados están accesibles para descargar con los siguientes enlaces:=,=a>>Descargar plantilla ejercicio 1>>Algoritmos no lineales PD1-1.xlsx>>d=,=br=,=a>>Descargar plantilla ejercicio 2>>Algoritmos no lineales PD1-2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"Árboles de decisión en KNIME",        
        "descripcion":"Estudiaremos la construcción de un modelo de árbol de decisión de regresión simple en la herramienta KNIME.",
        "contenido":"p>> Se descargó el tutorial de árbol de decisión simple KNIME de =,=a>>https://www.knime.com/nodeguide/analytics/regressions/learning-a-simple-regression-tree>>https://www.knime.com/nodeguide/analytics/regressions/learning-a-simple-regression-tree=,=br=,=img>>UT4-PD2-1.jpg>>800>500=,=h3>>Análisis=,=br=,=h5>>Descripción=,=p>> Este workflow utiliza el dataset Iris dividiéndolo con muestreo estratíficado en una parte de entrenamiento de un 80% y otra de prueba de un 20% para el caso de un modelo de árbol de regresión simple del cual se obtiene la performance y se la visualiza a través de una es gráfica. En este modelo a partir del ancho y el largo del sépalo, el largo de los pétalos y la clase de una flor entre Iris-setosa, Iris-versicolor y Iris virgínica predice el ancho de sus pétalos. =,=h5>> Componentes =,=h6>>File reader=,=p>>El operador equivalente a FileReader en RapidMiner es Retrieve. Estos se diferencian en que el primero deja editar características del dataset luego de ser incluido en el proyecto como los tipos para las variables y en RM esto se configura solo al importar el dataset. En el dataset sepal length, sepal width, petal length y pethal width son doubles y class es una string.=,=img>>UT4-PD2-2.jpg>>500>>650=,=br=,=br=,=h6>>Partitioning=,=p>> El operador partitioning ofrece elegir el tamaño de la partición y la forma en la que se quiere separar determinando por ejemplo si se eligen los primeros valores o si se hace muestreo estartíficado, da la opción para usar una seed como en RM, permite alternativas acerca de las Flow Variables que funcionan para a hacer variar ciertas configuraciones en el nodo de forma dinámica con cada ejecución y deja a elección políticas sobre el uso de la memoria. =,=p>> El operador equivalente a Patitioning en RapidMiner es Split. La principal diferencia es que mientras que en el componente de KNIME se hace una partición por cada unidad del componente, en RM se permite realizar varias a la vez.=,=img>>UT4-PD2-3.jpg>>500>>400=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> El proceso utiliza un algoritmo base de árbol de regresión simple siguiendo el algoritmo descripto en “Classification and Regression Tress” (Breiman et al, 1984) con algunas simplifaciones como no pruning, no necesariamente árboles binarios, tratar de encontrar la mejor dirección para los missing values, etc. En estos árboles de regesión el valor de predicción el valor para cada nodo hoja es la media de los registros dentro de ella y la predicción mejora cuanto menor sea la varianza de los valores dentro de una hoja. Por lo tanto, para armarlo en cada nodo se hacen splits que minimicen la suma de errores cuadráticos de los hijos. El operador Simple Regression Tree Learner soporta predictores de tipo numérico y categórico aunque solo soporta target columns de tipo numérico.=,=p>> Los parámetros que se pueden configurar del algoritmo son determinar el uso o no splits binarios para los atributos nominales, la forma en la que se manejan los missing values siendo XGBoost un algoritmo que calcula la mejor dirección para los valores faltantes y Surrogate que calcula para cada Split otros alternativos que mejoran la aproximación, el límite para la profundidad del árbol, el mínimo de valores que puede tener un nodo para que el Split se intente y el mínimo de registros que un nodo hijo puede tener.=,=img>>UT4-PD2-4.jpg>>700>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> Este operador recibe por un lado el modelo entrenado y por otro los datos de test, cómo salida tiene las predicciones realizadas. Permite modificar manualmente la columna de predicción, utlizar Flow Variables y decidir sobre políticas de memoria. =,=img>>UT4-PD2-5.jpg>>400>>300=,=br=,=br=,=h6>>Line Plot=,=p>> Este componente muestra una gráfica que compara los valores de predicción con la salida conocida para esos inputs del dataset de entrenamiento.=,=p>> Los parámetros que se pueden editar son el número de filas a mostrar, el límite del valor nominal a partir del cual una columna sea ignorada y la opción de colocar Flow Variables.=,=img>>UT4-PD2-6.jpg>>400>>300=,=img>>UT4-PD2-7.jpg>>500>>500=,=br=,=br=,=h6>>Numeric Scorer=,=p>> El operador funciona realizando los cálculos para los valores de coeficiente de terminación, media de error absoluto, error cuadrático medio y desviación media con signo de la predicción realizada.=,=img>>UT4-PD2-8.jpg>>500>>400=,=img>>UT4-PD2-9.jpg>>250>>125"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"Comparación de Decision Trees en herramientas de ML",
        "descripcion":"Comparación de árboles de decisión entre RapidMiner y Weka aplicados al dataset Iris. Además se analiza el componente mencionado en Azure ML Studio, KNIME y Python Sci kit learn.",
        "contenido":"p>>Se utiliza el dataset Iris y las herramientas RapidMiner y Weka para hacer una comparación de la performance de sus modelos de árboles de decisión midiéndola con un Split de datos de un 70% y un 30% para entrenamiento y test respectivamente en ambas.=,=h4>>RapidMiner=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión. =,=h6>>Parámetros=,=p>>- Criterion: gain_ratio. Selecciona el criterio que se usará para seleccionar los atributos sobre los que hacer los splits.=,=p>>- Maximal Depth: 10. La máxima profundidad del árbol.=,=p>>- Apply pruning: Activado. Si se aplica pruning o no.=,=p>>- Confidence: 0.1. Nivel de confianza usado para el error pesimista del cálculo de pruning.=,=p>>- Apply prepruning: Activado. Si se aplica prepruning o no=,=p>>- Minimal gain: 0.01. La ganancia de un nodo se calcula antes del Split. Se hace el Split si la ganancia es mayor al minimal gain.=,=p>>- Minimal leaf size: 2. Tamaño mínimo de observaciones por hoja.=,=p>>- Minimal size for split: 4. El tamaño de un nodo es el número de ejemplos en él. Solo se hace el Split para obtener nodos con un tamaño mayor al minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Numero de nodos alternativos testeados para un Split cuando el prepruning previene un Split.=,=br=,=h4>>Weka=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- batchSize: 100. Numero de instancias a procesar si la predicción batch está siendo realiza.=,=p>>- Debug: False. Si es verdadero el clasificador podría poner info adicional como salida en consola.=,=p>>- doNotCheckCapabilities: False. Si es verdadero las capacidades del clasificador no son checkeadas antes de la compilación=,=p>>- initialCount: 0.0. Valor inicial del contador de la clase.=,=p>>- MaxDepth: 10. Máxima profundidad del árbol, con -1 no hay restricción.=,=p>>- MinNum: 2.0. Mínimo peso total para las instancias de una hoja.=,=p>>- minVariancePrep: 0.001. Mínima proporción de la varianza de todos los datos que necesita estar en un nodo para que se haga un Split.=,=p>>- noPruning. False. Si se realiza pruning.=,=p>>- numDecimalPlaces. 2. Número de posiciones decimales para usar en la salida del modelo.=,=p>>- numFolds: 3. Determina el tamaño de los datos usados para pruning.=,=p>>- Seed. 1. La semilla usada para la aleatoriedad de los datos.=,=p>>- spreadIntialCount. False. Distribuir el recuento inicial en todos los valores en lugar de utilizar el recuento por valor.=,=br=,=h4>>Resultados=,=p>>Se obtuvo una performance de un 91.11% en el caso de RapidMiner y un 96% en Weka utilizando los parámetros que se muestran en la sección anterior para cada uno.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>100%>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>800>>800=,=br=,=br=,=h4>>Otras herramientas=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. Se utiliza una versión mejorada de los árboles de decisión con una gradient boosting machine.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- Maximum number of leaves per tree. Número máximo de hojas del árbol=,=p>>- Minimum number of samples per leaf node. Mínimo número de ejemplos en un nodo hoja.=,=p>>- Learning rate. Ritmo de aprendizaje.=,=p>>- Total number of trees constructed. Número de árboles construidos al entrenar el algoritmo.=,=p>>- Random number seed. Semilla de entrenamiento.=,=p>>- Allow unknown categorical levels. Seleccionado crea un nuevo nivel para cada atributo categórico.=,=br=,=h5>>KNIME=,=p>>Tipo de problema. Clasificación.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas o nominales. La variable objetivo solo puede ser nominal.=,=h6>>Parámetros=,=p>>- Class column. Selecciona la variable objetivo.=,=p>>- Quality measure. Para seleccionar la medida de calidad para la cual se calcularan los Splits. Las opciones son Gini index y Gain Ratio.=,=p>>- Pruning method. Pruning reduce el tamaño del árbol y evita el overfitting.=,=p>>- Reduced error pruning. Si se checkea se usa un simple método de pruning.=,=p>>- Min number records per node. Mínimo numero de registros requeridos por nodo.=,=p>>- Number records to store for view. Selecciona el número de registros guardados en un tree para la view.=,=p>>-  Average Split point. Al checkearla el valor para Split con atributos numéricos se determina según la media de los valores que separan a las dos particiones.=,=p>>- Number threads. Permite multiprocesamiento.=,=p>>- Skip nominal columns without domain information. Seleccionada las columnas nominales que no información de valores de dominio se saltean.=,=p>>- Force root split column. Seleccionada, el primer split es calculado en la columna elegida sin evaluar ninguna otra para posibles splits.=,=p>>- Binary nominal splits. Al seleccionarla a los atributos nominales se les hacen splits binarios.=,=p>>- Max nominal. Número máximo de valores nominales.=,=p>>- Filter invalid attribute values in child nodes. Habilitando esta opción se hace un post procesamiento del tree y se filtran checkeos inválidos.=,=p>>- No true child strategy: Opciones para cuando el valor de los atributos de un nodo es desconocido.=,=p>>- Missing value strategy. Opciones para los valores faltantes.=,=br=,=h5>>Python Sci kit learn=,=p>>Tipo de problema. Clasificación y regresión=,=p>>Algoritmo base. CART=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas. La variable objetivo puede ser nominal o numérica.=,=h6>>Parámetros=,=p>>- Criterio. Función para medir la calidad del Split. Puede ser Gini o entropy=,=p>>- Splitter. Estrategia utilizada para elegir el Split en cada nodo. Las opciones son mejor o random.=,=p>>- Max Depth. Máxima profundidad del árbol.=,=p>>- Min samples. Mínimo de ejemplos requeridos para hacer un Split generando un nodo interno.=,=p>>- Min samples leaf. Mínimo de ejemplos requeridos para hacer un Split generando un nodo hoja.=,=p>>- Min weight fraction. Fracción de peso mínimo del total de peso requerido en un nodo hoja.=,=p>>- Max features. Máximo número de variables de entrada considerado para hacer el mejor Split.=,=p>>- Random_state. Controla la aleatoriedad del estimador.=,=p>>- Max_leaf_node. Máximo número de nodos hoja.=,=p>>- Min_impurity_decrease: Se le hará un Split a un nodo si el Split da un decenso de la impureza mayor o igual a este valor.=,=p>>- Class_weight: Pesos asociados a las clases.=,=p>>- Ccp_alpha: Parámetro de complehidad usado para el pruning de mínimo costo-complejidad."
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"SVM lineal con descenso de sub gradiente",        
        "descripcion":"En una hoja de cálculo analizaremos el algoritmo de Support Vector Machines lineal con descenso de sub gradiente y lo utilizaremos para realizar predicciones.",
        "contenido":"h3>>Pasos=,=p>>- Se graficaron los datos en dos series para Y=1 y para Y=-1.=,=img>>UT4-PD3-1.jpg>>750>>250=,=br=,=br=,=p>>- Para hallar los coeficientes B1 y B2 del modelo SVM lineal B0 + B1 x X1 + B2 x X2 = 0 se utilizó el método del descenso de sub-gradiente. El coeficiente B0 fue descartado por lo cual la recta resultante pasa por el origen.=,=p>>Para aplicar este algoritmo se comienza con los coeficientes B1 y B2 en 0 y posteriormente se calcula un primer valor de salida con la fórmula dispuesta a continuación.=,=img>>UT4-PD3-2.jpg>>450>>50=,=p>>Si el valor de salida es mayor a 1 el patrón de entrenamiento no es un vector de soporte y por lo tanto se aplica la siguiente función para b1 y b2=,=img>>UT4-PD3-3.jpg>>250>>75=,=p>>En caso contrario se aplica la siguiente fórmula sobre los valores de los vectores utilizando en este caso lambda=0.45 y donde t es la iteración actual=,=img>>UT4-PD3-4.jpg>>550>>75=,=p>>- Este procedimiento fue realizado por 16 épocas en las que para cada una se itera sobre todo el dataset.=,=p>>- Se realizó una gráfica de la exactitud en función de las épocas obteniendo el siguiente resultado.=,=img>>UT4-PD3-5.jpg>>750>>350=,=br=,=br=,=p>>- Tras todas las iteraciones los coeficientes obtenidos fueron B1=0.55 Y B2=-0.72 obteniendo el plano 0.55xX1 -0.72xX2 = 0. Este se utilizó para calcular las predicciones con los datos de entrenamiento y se obtuvo un 100% de exactitud.=,=a>>Descargar planilla de trabajo>>Algoritmos no lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"SVM no lineal en RapidMiner",
        "descripcion":"En este ejercicio se analizará el componente SVM de RapidMiner y se lo utilizará para resolver un problema no separable linealmente.",
        "contenido":"p>>El operador SVM utiliza la implementación de Java de support vector machine mySVM de Stefan Rueping pudiendo ser usado para regresión y clasificación. Este fue empleado para resolver un problema no separable linealmente primero con los parámetros por defecto y luego se lo resolvió modificando el kernel a polinomial.=,=p>> A continuación se describen y muestran los valores de los parámetros utilizados del operador y luego los resultados de performance obtenidos para ambos casos. =,=h5>>Parámetros=,=p>>- Kernel type: Tipo de función kernel a utilizar en el algoritmo. Valor inicial dot, final polynomial.=,=p>>- Kernel cache: Fija el tamaño de cache para las evaluaciones kernel. Valor: inicial 200, final 200.=,=p>>- C: Es una constante de complejidad que fija la tolerancia a clasificación errónea, cuando más alta más suaves son los límites y cuanto baja estos son más duros. Si es demasiado alta puede dar overfitting y si es muy baja puede dar over generalization. Valor: inicial 0.0, final 0.0.=,=p>>- Convergence épsilon: Especifica la precisión de las KKT conditions. Valor: inicial 0.01, final 0.01.=,=p>>- Max iterations: Número máximo de iteraciones. Valor: inicial 100000, final 100000.=,=p>>- Scale: Si está activado los valores se escalan. Valor: inicial activado, final activado.=,=p>>- Lpos: Factor para constante de complejidad del SVM caso positivos. Valor: inicial 1.0, final 1.0.=,=p>>- Lneg: Factor para constante de complejidad del SVM caso negativos. Valor: inicial 1.0, final 1.0.=,=p>>- Épsilon: Constante de insensibilidad. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon plus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon minus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Balance cost: Adapta Cpos y Cneg al tamaño relative de las clases. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss pos: Usa pérdida cuadrática para desviación positiva. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss neg: Usa pérdida cuadrática para desviación negativa. Valor: inicial desactivado, final desactivado.=,=br=,=h5>>Resultados=,=br=,=h6>>Caso inicial=,=img>>UT4-PD4-1.jpg>>900>>150=,=br=,=br=,=h6>>Caso final=,=img>>UT4-PD4-2.jpg>>900>>150=,=br=,=br=,=a>>Descargar proceso de RapidMiner>>UT4-TA7.rmp>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"Naive Bayes en planilla electrónica",
        "descripcion":"Implementación de un modelo de Naive Bayes en una planilla electrónica.",
        "contenido":"p>>El modelo Naive Bayes para clasificación en problemas binarios o multiclase se caracteriza por asumir que los atributos son independientes entre sí (lo cual rara vez acontece) y en basarse en el teorema de Bayes.=,=h6>>Teorema de Bayes=,=p>>Este es utilizado para calcular probabilidad de una hipótesis dado un suceso de la siguiente manera.=,=img>>UT4-PD6-1.jpg>>350>>100=,=p>>Siendo=,=p>>- P(h|d) la probabilidad de la hipótesis h dado el susceso d (conocida como la probabilidad a posteriori).=,=p>>- P(d|h) la probabilidad del suceso d dado que la hipótesis h sea cierta.=,=p>>- P(h) la probabilidad de que la hipótesis h sea cierta (conocida como la probabilidad a priori).=,=p>>- P(d) la probabilidad de que suceda el suceso d más allá de que se cumpla h o no.=,=p>>En Naive Bayesian se calcula las probabilidades a priori para cada clase, las probabilidades para cada combinación entre un valor de un atributo y una clase, se emplea el Teorema de Bayes para obtener la probabilidad a posteriori y finalmente se clasifica según la clase que produjo el valor más elevado.=,=br=,=h4>>Ejercicio=,=p>>Empleando el dataset Jugar Tenis se crea un modelo Naive Bayes que luego es utilizado para predecir si se juega o no al tenis en condiciones metereológicas particulares definidas por los atributos de los datos.=,=h5>>Dataset=,=img>>UT4-PD6-2.jpg>>450>>300=,=br=,=br=,=p>>Se calcularon las siguientes probabilidades para generar el modelo tal y como se describió anteriormente pero para este caso particular.=,=img>>UT4-PD6-3.jpg>>650>>450=,=br=,=br=,=p>>Posteriormente se utilizó el modelo generado para realizar las predicciones que se encuentran a continuación.=,=img>>UT4-PD6-4.jpg>>650>>100"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"KNN en hoja de cálculo y RapidMiner",
        "descripcion":"Empleo de KNN para realizar predicciones en una hoja de cálculo y RapidMiner.",
        "contenido":"h2>>Ejercicio 1=,=p>>Aplicaremos el algoritmo KNN utilizando distancia euclideana en una hoja de cálculo.=,=h3>>Pasos=,=p>>- Graficamos de los datos en dos series para Y=0 y para Y=1=,=img>>UT4-PD8-1.jpg>>650>>200=,=br=,=br=,=p>>- Agregamos el punto con (x1, x2)= (8.093607318, 3.365731514) para clasificarlo usando diferentes valores de k.=,=p>>K=3: Puntos más cercanos: (x1, x2) = (7.423436942, 4.696522875), (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>K=2: Puntos más cercanos: (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>K=1: Puntos más cercanos: (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=br=,=p>>- Añadimos el punto con (x1, x2)= (5, 2.5) para clasificarlo usando diferentes valores de k.=,=p>>K=3: Puntos más cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (3.110073483, 1.781539638), (x1, x2) = (5.745051997, 3.533989803). Predicción Y = 0.=,=p>>K=2: Puntos más cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (5.745051997, 3.533989803). Predicción Y = N/A.=,=p>>K=1: Puntos más cercanos: (x1, x2) = (5.745051997, 3.533989803). Predicción Y = 1.=,=a>>Descargar planilla electrónica>>KVecinosMasCercanos.xlsx>>d=,=br=,=br=,=h2>>Ejercicio 2=,=p>>Aplicaremos el algoritmo KNN en RapidMiner para el dataset Iris.=,=h4>>Modelo=,=img>>UT4-PD8-2.jpg>>700>>300=,=br=,=br=,=h4>>Gráfica=,=img>>UT4-PD8-3.jpg>>100%>>450=,=br=,=br=,=h4>>Consideraciones=,=p>>Se puede observar que los datos correspondientes a la clase iris-setosa se encuentran bastante separados de los de las otras dos. En el caso de estos últimos iris-versicolor e iris-virginica, si bien tienen un poco de solapamiento también se pueden ver áreas diferenciables.=,=h4>>Preparación de datos=,=p>>Es conveniente estandarizar los datos con un range transformation entre 0 y 1.=,=h4>>Operador KNN en RapidMiner=,=p>>Voto ponderado: Si se activa este parámetro los valores de distancia entre los ejemplos se tienen en cuenta para la predicción. Es útil para ponderar las contribuciones de los vecinos de forma tal que los vecinos más cercanos contribuyan más que los más lejanos.=,=p>>Tipos de medición: Este parámetro se utiliza para seleccionar el tipo de medida que se utilizará para encontrar los vecinos más cercanos. Las opciones son las siguientes.=,=p>>- MixedMeasures: Se utiliza para calcular distancias en el caso de atributos tanto nominales como numéricos.=,=p>>- NominalMeasure: Se usa para el caso de solo atributos nominales.=,=p>>- NumericalMeasure: Se usa para el caso de solo atributos numéricos.=,=p>>- BregmannDivergences: Se selecciona para emplear diveregencias de Bregmann como tipos de medidas de cercanía.=,=p>>Funciones de medición:=,=p>>- EuclideanDistance. Dist = Sqrt ( Sum_(j=1) [y(1,j)-y(2,j)]^2).=,=p>>- CanberraDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)| / (|y(1,j)|+|y(2,j)|).=,=p>>- ChebychevDistance. Dist = max_(j=1) (|y(1,j)-y(2,j)|).=,=p>>- CosineSimilarity. Mide el coseno del ángulo entre los vectores de atributos de los dos ejemplos.=,=p>>- DiceSimilarity. La DiceSimilarity para atributos numéricos se calcula como 2 * Y1Y2 / (Y1 + Y2). Y1Y2 = Suma sobre el producto de valores = Suma_ (j = 1) y (1, j) * y (2, j). Y1 = Suma de los valores del primer Ejemplo = Suma_ (j = 1) y (1, j) Y2 = Suma de los valores del segundo Ejemplo = Suma_ (j = 1) y (2, j).=,=p>>- DynamicTimeWarpingDistance. Se calcula la distancia en una ruta de deformación óptima desde el vector atributo del primer ejemplo al segundo ejemplo.=,=p>>- InnerProductSimilarity. Dist = -Similarity = -Sum_(j=1) y(1,j)*y(2,j).=,=p>>- JaccardSimilarity. Dist = Y1Y2/(Y1+Y2-Y1Y2).=,=p>>- KernerlEuclideanDistance. La distancia se calcula mediante la distancia euclidiana de los dos Ejemplos, en un espacio transformado. La transformación está definida por el kernel y los parámetros correspondientes elegidos.=,=p>>- ManhattanDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)|.=,=p>>- MaxProductSimilarity. Dist = -Similarity = -max_(j=1) (y(1,j)*y(2,j)).=,=p>>- OverlapSimilarity. Dist = minY1Y2 / min (Y1, Y2).=,=h4>>Resultados=,=p>>K=3, euclidean distance:=,=img>>UT4-PD8-4.jpg>>800>>150=,=p>>K=2, euclidean distance:=,=img>>UT4-PD8-5.jpg>>800>>150=,=p>>K=3, manhattan distance:=,=img>>UT4-PD8-7.jpg>>800>>150=,=p>>K=2, manhattan distance:=,=img>>UT4-PD8-6.jpg>>800>>150=,=br=,=br=,=h4>>Conclusiones=,=p>>Con distancia euclideana, al variar el valor de k se obtuvieron diferentes valores de performance, pero esto no sucedió en los ejemplos realizados para la distancia manhattan.=,=a>>Descargar proceso de RapidMiner>>UT4-TA8.rmp>>d"
    },
    {
        "unidad":"Caso",
        "titulo":"Enfermedad Cardíaca",
        "descripcion":"",
        "contenido":"br=,=img>>cardiaca-0.jpg>>100%>>600=,=br=,=hr=,=br=,=h2>>Contexto=,=hr=,=p>>Las enfermedades cardiovasculares son la principal causa de muerte en el mundo según la Organización Mundial de la Salud (OMS) llevándose un estimado de 17.9 millones de vidas cada año lo cual supone un 32% de todas las muertes anuales alrededor del planeta. En el Uruguay según datos oficiales del Ministerio de Salud Pública (MSP), para cada año del quinquenio (2015-2019)  anterior a la pandemia del SARS-CoV2, conocido coloquialmente como coronavirus, estas enfermedades motivaron entre un 25% y un 27% de la totalidad de decesos conforme a lo que pudo identificar nuestro sistema de salud.=,=p>>Entre las principales causas de las enfermedades coronarias identificadas por el Centro de Control y Prevención de Enfermedades de los Estados Unidos (CDC) se encuentran alta presión arterial, colesterol elevado, fumar, diabetes, sobrepeso u obesidad, dietas poco saludables, poca actividad física y excesivo uso consumo de alcohol.=,=p>>Por otra parte, según datos del CDC se observa que esta enfermedad se da con mayor frecuencia para los hombres que para las mujeres con un 13.6% frente a un 8.4% de las respectivas poblaciones habiendo reportado padecimientos coronarios y que también se da con más ocurrencia al envejecer llegando hasta la franja a partir de los 65 años en la que un 17% de esa población reportó tenerla.=,=p>>La mayor sugerencia realizada por el sector de la salud al respecto de los padecimientos del corazón es tratar de prevenirlos lo máximo posible teniendo una vida saludable. De todas maneras, existen tratamientos como prescripción de cambios de rutina, medicación y hasta procedimientos quirúrgicos dependiendo de los diversos factores que estén causando la enfermedad, pero para todos ellos es fundamental tener un diagnóstico acertado y a tiempo con un médico.=,=p>>=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>En este caso de estudio se desarrollarán todos los pasos CRISP-DM para la problemática supervisada y de clasificación acerca de la predicción de enfermedades cardíacas con fines académicos, sin que exista inicialmente la finalidad de poner en producción el resultado conseguido. A continuación se pasará por las etapas de entendimiento de datos, preparación, modelado y evaluación para la realización de predicciones acerca de si un paciente cuenta o no con una enfermedad del corazón a partir de algunas de sus características empleando para la herramienta RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para el trabajo se cuenta con cuatro bases de datos vinculadas al diagnóstico de enfermedades del corazón y cualidades de los pacientes provenientes respectivamente de V.A Medical Center (California, Estados Unidos) autor: Robert Detrano M.D. Ph.D,  Cleveland Clinic Foundation (Ohio, Estados Unidos) autor: Robert Detrano M.D. Ph.D, University Hospital (Zurich, Suiza) autor: William Steinbrunn M.D y Hungarian Institute of Cardiology (Budapest, Hungría) autor: Andras Janosi M.D.=,=p>> Los datos fueron analizados en la herramienta RapidMiner y para ello hubo que realizarles un pre procesamiento en Python armando documentos en formato .csv con un formato con una cabecera conteniendo los nombres de los atributos y los registros de un paciente por fila a partir de cada uno de los cuatro archivos originales individualmente en los que los datos están separados por espacios y en varios renglones para cada individuo del cual se cuenta con información.=,=h6>>Descargas:=,=a>>Descripción de datos>>cardiac description.txt>>d=,=br=,=a>>Datos>>cardiac data.zip>>d=,=br=,=br=,=p>>Los datasets cuentan con una totalidad de 920 ejemplos etiquetados para los cuales hay 76 atributos numéricos que especifican diversas características de los pacientes entre los que se encuentra la variable objetivo del problema llamada num la cual representa el diagnóstico de enfermedad cardíaca. Esta última tiene un valor de 0 si no hay presencia de enfermedad en el individuo y valores del 1 al 4 según diferentes tipos de padecimientos del corazón.=,=p>>A pesar de la gran cantidad de atributos 10 de ellos no fueron utilizados, tal y como se expresa en la descripción de los datasets, siendo estos thalsev, thalpul, earlobe, lvx1, lvx2, lvx3, lvx4, lvf, cathef y junk. Por otra parte, hay un gran número de predictores con valores faltantes los cuales vienen represntados con el valor -9. Existen 20 de ellos con más de un 50% de missing values respecto a la totalidad de los registros de los cuales 15 no se encuentran en la clasificación vista anteriormente y son pncaden, restckm, exerckm, exeref, exerwm, restef, restwm, dm, smoke, ca, om2, ramus, diag, thal y thaltime.=,=br=,=h2>>Preparación de datos=,=hr=,=p>>Lo primero que se realizó fue importar los datos correspondientes a las cuatro bases en la herramienta RapidMiner comprobando que todos se cargaban con los mismos tipos para cada uno de sus atributos. Posteriormente se procedió a juntarlas utilizando el componente Append y obteniendo así el conjunto con todos los datos etiquetados.=,=p>>Se declaró a los registros con valor -9 como missing values y primeramente se descartaron las columnas con más de 50% de valores faltantes considerando que la mayor parte de estas concierne atributos no utilizados según lo expresado en la descripcion de los datos y además a que estas pueden generar dificultades para el aprendizaje de los modelos a crear, los restantes fueron reemplazados por el promedio de los datos correspondientes a cada predictor según cada uno de los caso.=,=p>>Otros procedimientos realizados fueron la remoción de outliers utilizando los operadores Detect Outliers y Filter Examples para quitar los 10 outliers más lejanos de los 10 vecinos más cercanos según distancia euclideana, la estandarización de todos los predictores con el componente Normalize y el cambio de tipo de la variable objetivo de numérica a polinomial usando Numerical to Polynomial.=,=br=,=h2>>Modelado=,=hr=,=p>>Para los modelos a desarrollar se consideraron aquellos supervisados y de clasificación que soportaran variables polinomiales como Naive Bayes, Árboles de Decisión, K-Nearest Neighbors y los algoritmos de ensamble siendo estos uno de tipo bagged como Random Forest y otro de boosting como Gradient Boosted Trees.=,=p>> De los estudiados fueron descartados K-Nearest Neighbors debido a la alta dimensionalidad de los datos, Árboles de Decisión al incluir Random Forest porque el segundo es una evolución del primero por lo que dificilmente se obtengan buenos resultados en el descartado sin conseguirlos también para el otro y Gradient Boosted Trees se eligió ante la posibilidad de obtener un modelo con overfitting.=,=p>>Por lo tanto, se eligieron los modelos de Naive Bayes y Random Forest, con un modelo teniendo parámetros predeterminados y otro con optimizados para este último, aplicando en todos los casos Cross Validation de 10 folds y midiendo sus performances.=,=h6>>Proceso en RapidMiner=,=img>>cardiac-0.jpg>>100%>>450=,=a>>Descargar proceso>>Heart Disease.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Para evaluar los modelos y medir su rendimiento se estudiarán las matrices de confusión. Las tablas obtenidas fueron los siguientes.=,=h6>>Naive Bayes=,=img>>cardiac-1.jpg>>100%>>200=,=br=,=br=,=h6>>Random Forest estándar=,=img>>cardiac-2.jpg>>100%>>200=,=p>>Los valores de optimización son number of trees = 100, criterion = gain_ratio y maximal depth = 10.=,=h6>>Random Forest optimizado=,=img>>cardiac-3.jpg>>100%>>200=,=p>>Los valores de optimización son number of trees = 100, criterion = gain_ratio y maximal depth = 30.=,=br=,=h2>>Conclusiones=,=hr=,=p>>Logramos observar que los algoritmos de ensamble dan los mejores resultados de accuracy y aún más aquellos el que tiene sus parámetros optimizados pudiendo de esta manera visualizar como vale la pena optimizar los parámetros de los modelos aunque esto resulte más costoso en cuanto a tiempo y recursos especialmente considerando la importancia del modelo realizado al estar vinculado a algo tan sensible como las enfermedades del corazón. En base a todo lo anterior se logra ver que de cierta forma existe una relación inversa entre performance global y el tiempo de ejecución de los modelos realizados en el caso de estudio.=,=p>>Si bien la performance global es muy importante, para este caso particular resulta de particular relevancia la presición para la clase 0 representando esta el pocentaje de acierto en la predicción del diagnóstico de los casos en los que no hay casos enfermedad lo cual es fundamental porque no se pueden permitir altos valores de falsos positivos en problemas tan sensibles como la salud del corazón. En esto se puede destacar el Random Forest optimizado es el que tiene la presición más alta para la clase 0 aunque Naive Bayes se encuentra muy cerca y a su vez es el que tiene menos recall para esta identificando la menor cantidad de personas que no tienen enfermedades cardíacas como que si las tuvieran de entre los modelos realizados."
    },
    {
        "unidad":"Caso",
        "titulo":"Ames Housing",
        "descripcion":"",
        "contenido":"br=,=img>>ames-0.jpg>>100%>>500=,=p>>Ames Iowa, Estados Unidos=,=hr=,=br=,=h2>>Contexto=,=hr=,=p>>La industria de bienes raíces o real estate es una de las que genera más actividad y crecimiento en el mundo. En los Estados Unidos según datos anualizados del Bureau of Economic Analyisis (BEA) para el segundo cuarto de 2021 el sector vinculado a la vivienda es una de las industrias que genera mayor actividad económica en dicho país con un PIB de 2.908 trillones de dólares. Conforme datos también anualizados del Fondo Monetario Internacional de octubre 2021, esta cifra supera el PIB de países como España, Italia, Brasil, Australia, Canadá, Rusia, Corea del Sur y equivale a 48.38 veces el de Uruguay.=,=p>>El valor de compra de una propiedad depende de muchos factores entre los cuales diversas fuentes destacan los siguientes.=,=p>>Localización: Uno de los principales está vinculado a la famosa frase de la industria de bienes raíces “location, location, location” haciendo referencia a que gran parte del valor de un inmueble depende la calidad de oportunidades y servicios que se puedan obtener viviendo en ese lugar como por ejemplo posibilidades de obtener un buen empleo, seguridad, proximidad a escuelas de calidad, etc.=,=p>>Espacio: Es un factor fundamental para determinar su valor haciendo especial énfasis en el espacio utilizable que es aquél en el que no se cuenta el área correspondiente aáticos, sótanos y los lugares que las personas rara vez utilizan en su día a día. Muchas personas eligen vivir en suburbios en los que pueden disponer de interiores, jardines o fondos más amplios que vivir en el centro de la ciudad en una inmueble más pequeño por más que esto le quite la posibilidad de acceder a alguna oportunidad o servicio con la misma facilidad. Esto se puede ver en gran extensión en países como Estados Unidos o Canadá, pero también en Uruguay con Ciudad de la Costa.=,=p>>Formato: Los compradores están dispuestos a pagar por conseguir una vivienda que sea acorde a sus necesidades actuales o a sus perspectivas de futuro como sería el caso de una familia que tiene expectativas de tener más hijos y busca una casa lo suficientemente grande para ello. Dentro de estas características se destacan algunas como el número de habitaciones disponibles, la cantidad de baños y si el inmueble tiene garage o no. Por otra parte, el formato en si de la vivienda está relacionado fuertemente a su precio, no es lo mismo una casa que un apartamento con todas las variantes que estos pueden tener.=,=p>>Estado: El estado de una propiedad es un factor fundamental para su precio siendo esta es la razón por la cual es conveniente económicamente reacondicionar una propiedad antes de venderla. Por otra parte, a mayor antigüedad las construcciones son más propensas a tener defectos y por lo tanto se valora la modernidad.=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>Este caso de estudio tiene como finalidad poder desarrollar el modelo CRISP-DM sobre un problema con fines académicos no teniendo como objetivo un deployment o que forme parte de un producto más grande. Se realizará preparación de datos, modelado y evaluación para predecir el valor de viviendas a partir de características de las propiedades o de sus entornos utilizando Python y librerías como SciKitLearn, Pandas, Numpy, Seaborn, entre otras. El código completo al que se hace referencia en este trabajo se encuentra disponible en la última sección de Anexo.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para este proyecto se cuenta con datos de venta de propiedades de la localidad de 66.000 habitantes llamada Ames del estado de Iowa Estados Unidos entre los años 2006 y 2010 obtenidos del Ames City Assessor´s Office y preprocesados por Dean De Cock PhD en Probabilidad de la Iowa State University. Los registros se encuentran en dos datasets, uno de training y uno de test.=,=h6>>Descargas:=,=a>>Descripción de datos>>data_description.txt>>d=,=br=,=a>>train.csv>>train.csv>>d=,=br=,=a>>test.csv>>test.csv>>d=,=br=,=br=,=p>>La estructura de los datasets a utilizar tiene 78 predictores más la variable objetivo numérica continua en el caso del de train la cual es la que se querrá predecir con el modelado a desarrollar, tratándose así de un problema supervisado de regresión. De las variables de entrada es muy importante tener en consideración sus diferentes tipos contando con 44 categóricas siendo 23 nominales y 21 ordinales además de 34 numéricas con 19 continuas y 15 discretas.=,=p>> Aunque las variables nominales y ordinales son categóricas se diferencian en que las primeras son ordenables mientras que las segundas son simplemente características cuyos valores dificimente son comparables entre ellos en el sentido de que uno se pueda considerar mayor o menor que el otro de por sí. Una mejor explicación se podría dar considerando atributos de este problema en particular como es el caso del ordinal ExterQual y del nominal Foundation. ExterQual hace referencia a la calidad de los materiales del exterior de la casa y sus valores posibles son Ex (excelente), Gd (bueno), TA (promedio), Fa (justo), Po (pobre) por lo cual es fácil de trasladar a una escala en la cual los primeros valores son mayores a los últimos dado que es indiscutible que en cualquier caso una calidad excelente es mejor que una promedio o una pobre. Por otra parte, un predictor como Foundation que describe a los cimientos de una casa contando con las categorías BrkTil (ladrillos y tejas), CBlock (bloque de cemento), PConc (concreto), Slab (losa), Stone (piedra) y Wood (madera) dificilmente pueda ser ordenable dado que en algunos terrenos podría ser mejor contar con cimientos de un tipo y en otros de otro con lo que el orden dependería de otras circunstancias generandose así una la dificultad para establecer un orden absoluto para esta categoría=,=p>>En cuanto a la cantidad de ejemplos, hay 1460 datos etiquetados en el train y 1459 sin etiquetar en el test. El caso de estudio se enfocará en trabajar con el de entrenamiento dado que este nos permite medir la performance al desarrollar modelos de predicción comparando los valores obtenidos por los mismos con las etiquetas brindadas. De esta forma todo el análisis que se realizará a continuación refiere a ese conjunto de datos. =,=p>>Analizando los datos faltantes se puede notar que son 8 y que todos corresponden a la variables continua MasVnrArea la cual hace referencia a las dimensiones del área de revestimiento de mampostería en pies cuadrados de una propiedad. A proósito, es conveniente destacar que en muchas variables categóricas existe la palabra NA como uno de los valores posibles siendo un ejemplo de ellos GarageType en cuyo caso se utiliza para referir a que las propiedades que no tienen garage. Estas observaciones con NA no deben ser confundidas con datos faltantes.=,=p>>Es interesante observar la correlación que estos mantienen tanto con la variable objetivo como entre sí. Para ello se observara la matriz de correlación generada a partir del coeficiente de Pearson que mide la correlación lineal entre dos variables en una escala entre -1 y 1 interpretándose como cuanto más cerca del cero menor dependencia lineal y cuanto más lejos mayor ya sea directa en el caso de la aproximación al 1 u opuesta en los acercamientos al -1. Este solamente puede ser calculado para las variables numéricas y las ordinales tras convertirlas a una escala numérica, no tiene sentido buscar este tipo de relaciones para las nominales dado que no se podría establecer una dependencia de tipo lineal entre dos variables siendo una de estas no ordenable.=,=p>>Filtrando entre los atributos que tienen una correlación mayor a 0.5 o menor a -0.5 respecto a la variable objetivo según Pearson se obtienen de forma ordenada los siguientes 13 predictores.=,=img>>ames-1.jpg>>200>>250=,=br=,=br=,=p>>A continuación se calculan las correlaciones lineales entre ellos mostrandose en esta matriz de Pearson.=,=img>>ames-2.jpg>>950>>375=,=br=,=br=,=p>>Las conclusiones que se pueden sacar de estas dos imágenes son bastantes.=,=p>>Primero que nada es interesante observar que si bien se realizaron los cálculos para las correlaciones con la variable objetivo con 55 predictores entre los numéricos y los ordinales tras haber sido codificados para que también lo sean, solamente 13 tienen una predicción fuerte con esta lo cual es un 23% y de ellas todas son de forma directa y ninguna opuesta.=,=p>>En cuanto a las correlaciones entre los propios predictores se obtuvieron valores positivos en todos los casos con lo cual todos tienen una cierta dependencia lineal directa por más mínima que pueda ser. Se consideraron a los valores superiores a >0.66 como correlaciones altas lo cual se da entre los pares GrLivArea y TotRmsAvgGrd con 0.82, GarageArea y GarageCars con 0.88, TotalBsmtSF y 1stFlrSF con 0.81. Pensando estos tres casos es más comprensible entender las altas dependencias lineales entre sus factores. En el inicial es entre el espacio utilizable para la vivienda sobre el nivel del piso y la cantidad de habitaciones sobre el suelo lo que tiene sentido porque a más o menos cuartos generalmente se utiliza más o menos espacio respectivamente cumpliéndose de una forma que es aparentemente cercana a lineal, el del medio entre el espacio disponible del garage y su capacidad según la cantidad de autos que puede almacenar que es explicable naturalmente porque a más espacio más vehículos pueden ser guardados dentro y vice-versa que nuevamente parece mantener una relación casi lineal y en el último entre el tamaño del sótano y el de la planta baja podría explicarse porque en muchos casos estos dos pisos están estructurados uno sobre el otro generando también una relación próxima a la lineal.=,=p>>Estudiando las distribuciones de los atributos que tienen alta correlación con la variable a predecir y baja correlación entre sí se pueden observar outliers para GrLivArea en sus valores sobre 4000 (esto es expresado por el propio Dean De Cock en su paper sobre el dataset y el problema) y en TotalBsmtSF por sobre 3000 tal y como se muestra en las representaciones a continuación.=,=img>>ames-3.jpg>>400>>300=,=img>>ames-4.jpg>>400>>300=,=br=,=br=,=p>>Por otra parte, visualizando los histogramas se pudieron ver sesgos negativos en los atributos de YearBuilt y YearRemodAdd los cuales corresponden respectivamente al año de construcción y al de la última remodelación de las propiedades. Se puede ver que en el registro hay bastantes más datos de casas hechas o remodeladas hace pocos años con un 40% y un 59% de la muestra con año posterior a 1980 en cada caso.=,=img>>ames-5.jpg>>400>>300=,=img>>ames-6.jpg>>400>>300=,=br=,=br=,=p>>Para las variables nominales es útil analizar a sus distintas categorías en relación a la variable objetivo lo cual es posible con diagramas de dispersión o de caja.=,=p>>En los diagramas de dispersión se eligiö la variable continua con mayor correlación respecto a la variable objetivo llamada GrLivArea la cual representa el espacio utilizable de las propiedades para el eje X, SalePrice en el eje Y y los ejemplos de las observaciones con distintos colores según los diferentes valores posibles de la categoría que se está representando en cada caso. En estos si para distintos cifras de GrLivArea se logran distinguir agrupaciones de observaciones que corresponden a una misma categoría de la variable que se está representando a un cierto SalePrice y por lo tanto siguiendo un patrón diferente a las demás categorías del predictor, se podría sostener que este mantiene una relación con el precio de la vivienda y por lo tanto merece ser considerado tanto en el análisis, la preparación y el modelado de los datos.=,=p>>Cuando las categorías son demasiadas y esto dificulta distinguir grupos para las mismas en un diagrama de dispersión es bueno realizar digramas de caja respecto a la variable objectivo y ver como es la distribución de los datos de las diferentes categorías del predictor respecto al atributo a predecir y observar el rango, la mediana, los datos atípicos que tienen. De esta manera, si las cajas varían notoriamente a dierentes valores de SalePrice según categorías se puede afirmar que hay un cierto vínculo entre las dos variables graficadas que también merece la pena ser tenido en cuenta.=,=p>>Los diagramas más destacables se encuentran posteriormente. =,=p>>-HouseStyle representa el tipo de vivienda involucrada con códigos como 2Story, 1Story, 1.5Fin, 1.5Unf, 2.5Unf y 2.5Fin en referencia al número de plantas y a si están terminadas o no, por otra parte los valores SFoyer y SLvl son otras variantes de casas utilizadas en Estados Unidos. En su diagrama de dispersión lo primero que se puede es que las distintas categorías de la variable parecerían tener una relación lineal con la variable objectivo pero con diferentes pendientes. Además, a lo largo del eje de GrLivArea dado un cierto valor hay una especie de ordenamiento según las categorías teniendo por ejemplo a las de un piso costando más que las de dos y las de uno y medio.=,=img>>ames-14.jpg>>500>>300 =,=br=,=br=,=p>>-Foundation, como ya se explicó anteriormente, hace referencia a los tipos de cimientos de una casa. Nuevamente se puede ver en el scatter plot que las distintas categorías no tienen las mismas pendientes y que en este caso las propiedades con una cierta área disponible suelen costar más si tienen cimientos de concreto vertido a que si para ello se utilizaron bloques de cemento por dar un caso.=,=img>>ames-8.jpg>>500>>300 =,=br=,=br=,=p>>-CentralAir trata acerca de si el hogar cuenta con un aire acondicionado centralizado y se ve en las representaciones gráficas que las casas que no cuentan con uno valen menos que las que si cuando estas tienen la misma superficie utilizable creciendo ambas categorías con distinta pendiente otra vez.=,=img>>ames-9.jpg>>500>>300 =,=br=,=br=,=p>>-Neighborhood es el barrio del pueblo de Ames Iowa al que corresponde la vivienda. Como la cantidad de categorías es muy grande con 28 posibilidades resulta difícil analizar el scatter plot correspondiente y por tanto se optó por hacer un diagrama de caja. En este se puede ver como hay barrios con una media de precios más mucho más alta que otros, como en algunos los rangos de recios varían más y en otros menos, la falta de intersección, los datos atípicos de casas con precios exorbitantes. Con todo lo anterior se puede ver como hay barrios más ricos y más pobres y toda esta variabilidad muestra que este dato es muy importante para ser considerado posteriormente en el desarrollo del modelo tal y como se había analizado en un primer momento en el capítulo de contexto al averiguar que factores consideran los expertos que son más fundamentales al valorar una casa, en este caso se observa lo mismo.=,=img>>ames-13.jpg>>600>>400 =,=br=,=br=,=br=,=h2>>Preparación de datos=,=hr=,=p>>Los datos fueron cargados y se reemplazaron los valores con NA de las variables categóricas por una palabra Nope que significa que no hay eso de lo que trata el predictor lo cual puede tratarse de garages, sótanos, etc.=,=p>>Es central considerar como se explicó en el capítulo anterior son los diferentes tipos de predictores que se deben manejar. Se cuentan con atributos numéricos continuos y discretos y con categóricos nominales y ordinales. Se codificaron las variables categóricas pero teniendo en cuenta que debido a la cuestión del posible ordenamiento entre las categorías de los ordinales y no de los nominales hay que realizar dos tipos diferentes de codificación. Los atributos ordinales pueden transformar sus escalas a numéricas pasando de una a otra graduación que puede ser ordenada como hace el OrdinalEncoder() de SKLearn, lo importante aquí es mantener el ordenamiento y que no pase como en Python que codifica al revés y luego al calcular las correlaciones por ejemplo con la variable objetivo se obtiene el valor opuesto al que debería pero igualmente esto puede arreglarse fácilmente como se muestra en el anexo. En cuanto a las variables nominales que no siguen un orden se puede realizar lo que se llama one hot encoding que también se encuentra en SKLearn y en Pandas. Lo que hace es generar dummy variables que no son otra cosa que nuevos atributos binarios por cada uno de los diferentes valores que pueden serguir las variables nominales seleccionadas.=,=p>>Se codificaron las variables ordinales para hacer el estudio de las correlaciones de estas sumadas a las numéricas continuas y a las discretas tanto con la varaible objetivo como entre sí. Luego de esto nos quedamos con los predictores que mantuvieran una correlación superior a 0.5 o inferior a -0.5 respecto al atributo objetivo y con a su vez tengan una correlación con las demás que sea menor a 0.66 y mayor a -0.66 en todos los casos. De esta forma se seleccionaron los atributos OverallQual que refiere a la calidad general, GrLivArea que explica el área disponible para vivir, GarageCars la cantidad de vehículos que se pueden estacionar en el garage, ExterQual la calidad exterior de la propiedad, TotalBsmtSF el tamaño del sótano, BsmtQual la calidad del sótano, Kitchen Qual la calidad de la cocina, FullBath la cantidad de baños de la primera planta hacia arriba, YearBuilt el año de construcción y YearRemodAdd la fecha de remodelación. Los demás predictores numéricos o ordinales fueron descartados por no cumplir con condiciones de correlación que sean acordes para el problema según el algoritmo más restrictivo de los que se utilizarán.=,=p>>Posteriormente se observaron diagramas, ya sea de dispersiön o de caja según el caso, de los atributos nominales graficando SalePrice en función de GrLivArea como se mostró anteriormente. De esto se consiguieron los predictores HouseStyle, Foundation, CentralAir y Neighborhood los cuales también fueron descriptos con anterioridad. Estos atributos fueron seleccionados de entre los nominales y los demás se descartaron por no poder justificar que tengan una relación considerable con la variable objetivo que sea determinante para el problema que se trata en este caso de estudio.=,=p>>Tras haber seleccionado los atributos con los que se trabajará y fundamentándonos en el análisis hecho con antelación se prosigió a codificar con one hot encoding a sus variables nominales, eliminar sus outliers quitando los valores de GrLivArea<4000 y TotalBsmtSF>3000, resolver el skewness en los atributos YearBuilt y YearRemodAdd con transformaciones logarítmicas, verificar los datos faltantes, los cuales son nulos para todo el conjunto de datos resultante de la selección de atributos realizada, y a estandarizar las variables numéricas y ordinales.=,=p>>Por otra parte, se realizaron todas las transformaciones explicadas en los párrafos superiores a dos conjuntos de datos copia con la diferencia de que en uno no contenía variables categórgicas y el otro si pero solo ordinales y no nominales. Esto se hizo con el fin de poder medir el impacto en la performance que tiene agregarlas.=,=br=,=h2>>Modelado=,=hr=,=p>>En la decisión de los modelos a desarrollar es fundamental considerar el tipo de problema el cual en este caso consiste en predecir el valor de una propiedad a partir de un conjunto de predictores y por lo tanto se trata de uno supervisado y de regresión. Existen un conjunto de algoritmos que se pueden emplear en estas condiciones entre los cuales se encuentran regresión lineal considerando también sus variaciones Lasso y Ridge, árboles de decisión, random forests, gradient boosting regression, k-nearest neighbors, support vector regression, entre otros. Para decidir entre los modelos aplicables es necesario ver la estructura de los datos que tenemos en este problema en particular y que algoritmo es mejor en estas condiciones.=,=p>>Como se estudió en la sección de análisis de datos con los coeficientes de Pearson y los diagramas, los predictores mantienen una relación lineal con la salida lo cual se acentúa para los atributos seleccionados y trabajados en el data pre processing. De esta manera, como dicho algoritmo asume una relación lineal entre las entradas y la salida, aplicar regresión lineal para este caso es una buena por lo que será empleada a continuación. Se optará por no utilizar Lasso ni Ridge e ir por la versión convencional del algoritmo usando ordinary least squares sin modificaciones dado que los métodos de optimización son utilizados cuando hay gran multicorrelación entre los predictores y lo que se realizó con la selección de atributos fue reducir este factor.  =,=p>>El modelo seleccionado será entrenado con los predictores y datos pre procesados primero sin agregar variables categóricas, luego exceptuando a las nominales y finalmente con todas con ellas. Para estos se utilizará cross validation con 10 pliegos de forma tal de evitar el problema en el que la parte de testeo pueda llegar a ser particularmente más fácil o más dificil para uno, otro o ambos modelos como podría suceder con la técnica hold out.=,=br=,=h2>>Resultados=,=hr=,=p>>Para la medición de resultados se usarán dos coeficientes, el primero es RMSE o root mean squared error que mide la raíz cudrada del promedio de los cuadrados de las diferencias entre las predicciones y los datos etiquetados del dataset train al cuadrado por lo que cuanto menor mejor y el segundo es coeficiente de determinación R² el cual determina la calidad del modelo a partir de cuanta variabilidad en la variable dependiente puede ser explicada por las variables independientes brindando valores entre 0 y 1 y siendo un mejor modelo cuanto mayor sea su valor.=,=p>>Se consiguieron los sigiuientes coeficientes.=,=h6>>Regresión lineal=,=p>>- Con predictores numéricos y categóricos: RMSE = 24911, R² = 0.862=,=p>>- Con predictores numéricos y ordinales: RMSE = 26680, R² = 0.836=,=p>>- Con predictores sólo numéricos: RMSE = 28663, R² = 0.807=,=br=,=h2>>Conclusiones=,=hr=,=p>>=,=p>>En los resultados se puede visualizar que el modelo que dio mejor rendimiento para los tres conjuntos de datos fue el de regresión lineal y que seleccionar atributos tanto ordinales como nominales cuidadosamente y haciéndoles sus debidas transformaciones según el caso, vale la pena especialmente para el caso de la regresión lineal dado que esto optimizó los dos parámetros utilizados para medir la performance en las diversas situaciones para este modelo.=,=p>>Por otra parte, resulta interesante que a partir del desarrollo realizado tanto en el ánálisis de datos por ejemplo al ver la alta correlación que hay entre el tamaño disponible y el precio de la propiedad como en la preparación de datos y modelado siendo un caso conreto la selección de ciertas variables nominales que mejoran mucho el rendimiento de los modelos como es el caso del barrio, se logra ver como afectan en un caso real las características de las propiedades que los expertos mencionan como las más importantes lo que se analizó en un principio en la sección de contexto. Se puede ver como al incluir y procesar correctamente esos factores que ellos más destacan es lo que más termina determinando que al armar nuestros modelos estos hagan mejores predicciones.=,=br=,=h2>>Anexo=,=hr=,=iframe>>"
    }
]