[
    {
        "unidad":"CRISP-DM",
        "titulo":"PD3",
        "descripcion":"En este ejercicio se aplican técnicas de pre procesamiento y obtención de estadísticas para el dataset Wine utiliando funciones de Python.",
        "contenido":"h3>>Pasos=,=p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    dataset = list()=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    minmax = list()=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    train = list()=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas de los dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas de los dataset de train:\")=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0for i in range(10):    =,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>1200>>300=,=img>>UT2-PD3-2.jpg>>1200>>300=,=img>>UT2-PD3-3.jpg>>1200>>400=,=img>>UT2-PD3-4.jpg>>1200>>800=,=img>>UT2-PD3-5.jpg>>1200>>700"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD5",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python para realizar la clasificación a partir de conjuntos de datos.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión.=,=br=,=h3>>Código=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.linear_model import LogisticRegression=,=br=,=code>>.from sklearn.metrics import confusion_matrix, classification_report=,=br=,=code>>.from sklearn.model_selection import train_test_split=,=br=,=code>>.from sklearn import preprocessing=,=br=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>600>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100=,=p>>- Se transformaron los atributos string a números=,=p>>- Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equilvalente en la herramienta RapidMiner.=,=h3>>Código=,=code>> .import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.linear_model import LogisticRegression=,=br=,=code>>.from sklearn.metrics import confusion_matrix, classification_report=,=br=,=code>>.from sklearn.model_selection import train_test_split=,=br=,=code>>.from sklearn import preprocessing=,=br=,=code>>.=,=br=,=code>>.input_file = \"../sports_Training.csv\"=,=br=,=code>>.data_original = pd.read_csv(input_file, header=0)=,=br=,=code>>.#print(data_original.values)=,=br=,=code>>.=,=br=,=code>>.data = data_original[(data_original['CapacidadDecision'] >= 3) &(data_original['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.X = data[['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']].values=,=br=,=code>>.Y = data['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.#le = preprocessing.LabelEncoder()=,=br=,=code>>.#Y_encoded = le.fit_transform(data['DeportePrimario'].values)=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.input_file2 = \"../sports_Scoring.csv\"=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data = data_scoring[(data_scoring['CapacidadDecision'] >= 3) &(data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.Xsco = data[['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']].values=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=code>>.=,=br=,=code>>.print(\"Predicted\")=,=br=,=code>>.print(y_pred)=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>1200>>400=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 91.28% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Casos",
        "titulo":"Predicción de valores inmobiliarios",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> HELLO"
    },
    {
        "unidad":"Casos",
        "titulo":"Predicción de altura según restos óseos",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> chau "
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"TA1",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> hola =,=a>>hola>>https://google.com=,=hr=,=img>>prueba.jpg>>400>>400"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD2",
        "descripcion":"Esta PD consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> hola"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"TA2",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> hola"
    }
]