[
    {
        "unidad":"CRISP-DM",
        "titulo":"PD1",
        "descripcion":"En la siguiente práctica se realizaron tutoriales de RapidMiner para manejo de missing values, normalización y detección de outliers. Además utilizó esta herramienta para trabajar sobre normalización con el dataset Wine.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>1000>>200 =,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>1200>>200=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>>- Se creó un proceso en RapidMiner con dos canales, uno que utiliza el dataset normalizado con transforamción Z y uno que no.=,=p>>- En ambos canales se realizó un split con 70% de datos para entrenamiento y 30% para test. =,=p>>- Para cada caso se entrenó un modelo de Naive Bayes para clasificación por lo cual hubo que convertir el atriburto Wine de numérico a polinómico.=,=p>>- Se le midió performance del modelo.=,=br=,=h3>>Resultados=,=img>>UT2-PD1-3.jpg>>800>>300=,=br=,=a>>Descargar modelo>>UT2 PD1 ej2.rmp>>d=,=br=,=br=,=p>>Caso normalizado=,=img>>UT2-PD1-4.jpg>>1050>>200=,=p>>Caso sin normalizar=,=img>>UT2-PD1-5.jpg>>1050>>200=,=br=,=p>>Se obtuvo un poco de mayor presición para el caso del dataset normalizado lo cual se debe al algoritmo utilizado. Naive Bayes calcula frecuencias para hallar las clases de predicción y al estandarizar los datos, las frecuencias se mantienen de una forma similar."
    },
    {   
        "unidad":"CRISP-DM",
        "titulo":"PD2",
        "descripcion":"En este ejercicio se hicieron tutoriales de modelado, scoring, split y validación, cross validation y visual model comparison para la herramienta RapidMiner.",
        "contenido":"h3>> Modeling =,=img>>UT2-PD2-1.jpg>>800>>200 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>800>>200 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>900>>150 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>900>>200=,=img>>UT2-PD2-4-2.jpg>>900>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>800>>100=,=img>>UT2-PD2-5-2.jpg>>800>>125 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d"
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"PD3",
        "descripcion":"Para este caso se aplican técnicas de pre procesamiento como normalización, estandarización y split en datos de entrenamiento y test. Luego se obtuvieron estadísticas para el dataset Wine utiliando funciones de Python con SciKitLearn y la librería Pandas.",
        "contenido":"h3>>Pasos=,=p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0    dataset = list()=,=br=,=code>>.\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0    minmax = list()=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0    train = list()=,=br=,=code>>.\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.\u00a0for i in range(10):    =,=br=,=code>>.\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>1000>>300=,=img>>UT2-PD3-2.jpg>>1000>>300=,=img>>UT2-PD3-3.jpg>>1000>>300=,=img>>UT2-PD3-4.jpg>>1000>>600=,=img>>UT2-PD3-5.jpg>>1000>>600"
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"PD4",
        "descripcion":"Remoción de missing values, construcción de un modelo de predicción para el dataset Titanic y creación de un programa que responda a consultas sobre el dataset mencionado.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=br=,=p>> - Se importaron librerías y paquetes.=,=p>> - Se cargó el dataset y se los mostró.=,=p>> - Se gestionaron los datos faltantes.=,=p>> - Se graficaron los datos según Age vs Survived y Fare vs Survived..=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\"-----DATASET------\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----MISSSING VALUES------\")=,=br=,=code>>.print(\"*Antes de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Se reemplazan los missing values de edad por la media=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan las columnas body y cabin por altos porcentajes =,=br=,=code>>.# de missing values siendo estos 90.7% y 77% respectivamente=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan la fila sin valor para fare y las dos filas=,=br=,=code>>.# Sin valor para embarked=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#Se asigna una categoría única para los missing values de boay y home.dest=,=br=,=code>>.dataset[\"boat\"].fillna('U', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('U', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"*Despues de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----GRAFICAS------\")=,=br=,=code>>.print(\"- x = Age, y = Survived\")=,=br=,=code>>.print(\"- x = Fare, y = Survived\")=,=br=,=code>>.colors = (\"red\", \"blue\")=,=br=,=code>>.plt.scatter(dataset['age'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset['fare'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-1.jpg>>300>>600=,=img>>UT2-PD4-2.jpg>>400>>300=,=img>>UT2-PD4-3.jpg>>400>>300   =,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>> - Se calculó la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> - Se calculó la probabilidad de que un niño de 10 años o menos de 3ra clase sobreviva. =,=br=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilidades:\")=,=br=,=code>>.print(\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3)))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>>.print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>500>>200"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD1",
        "descripcion":"Utilizando una planilla electrónica se generó un modelo de regresión linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se hizo uso de los modelos para hacer predicciones.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se calcularon los coeficientes para un modelo de regresión lineal simple Y = B0 + B1 x X. Para ello se utilizarán las fórmulas:=,=img>>UT3-PD1-1.jpg>>600>>100=,=img>>UT3-PD1-2.jpg>>500>>50=,=p>> -Se aplicó el modelo a los valores de entrenamiento.=,=p>> -Se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gráfico para mostrar la recta de ajuste.=,=p>> -Se estimó el error de predicción RMSE con esta ecuación:=,=img>>UT3-PD1-3.jpg>>400>>100=,=p>> -Se utilizó un método alternativo para calcular B1 y a partir de este B0 al igual que antes. La fórmula utilizada fue la que se encuentra a continuación:=,=img>>UT3-PD1-4.jpg>>400>250=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>>-Se estimaron los coeficientes del modelo simple de regresión lineal Y = B0 + B1 x X realizando 24 iteraciones.=,=p>> -Se realizó el gráfico error de predicción contra iteraciones =,=p>> -Se calculó el error medio cuadrático RMSE.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD2",
        "descripcion":"En una planilla electrónica se visualizará una función logística y se genararán modelos de regresión logística con descenso de gradiente estocástico.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se crearon 10 valores para una variable X con distribución normal=,=p>> -Se aplicó la función logística a los anteriores con coeficientes elegidos arbitrariamente para hallar valores de una variable Y.=,=p>> -Se generó un gráfico para visualizar la función.=,=img>>UT3-PD2-1.jpg>>700>>300=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>> - A partir de un conjunto de datos se hizo una gráfica con dos series, una para Y=0 y otra para Y=1. =,=p>> - Se estimaron los coeficientes del modelo de regresión logística Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estocástico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 épocas. =,=p>> - Se calculó la predicción, el cuadrado del error, el error de clase para cada ejemplo de entrenamiento de cada época.=,=p>> - Se calculó la exactitud y el error medio cuadrático RMSE para cada época.=,=p>> - Se realizaron las gráficas exactitud vs época y RMSE vs época.=,=p>> - Se repitió todo el proceso anterior para un nuevo conjunto de datos.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD3",
        "descripcion":"Empleando una planilla electrónica se generará un modelo de análisis de discriminante lineal LDA y con él se hará la predicción de clase para un conjunto de datos con un atributo y una salida con dos clases.",
        "contenido":"h3>>Pasos=,=p>> - Se insertaron los valores del dataset. =,=p>> - Se graficaron los datos separando según las clases de salida. =,=p>> - Se calcularon, siendo n el número de clases, las medias para cada clase k y la varianza de x con las ecuaciones que se encuentran a continuación.=,=img>>UT3-PD3-1.jpg>>250>>75=,=img>>UT3-PD3-2.jpg>>350>>75 =,=p>> - Se calculó la predicción de clases usando: =,=img>>UT3-PD3-4.jpg>>400>200 =,=p>> - Se utilizó la siguiente ecuación para calcular los discriminantes de x para ambas clases Y = 0 e Y = 1.=,=img>>UT3-PD3-3.jpg>>500>>60=,=p>> -Se calculó finalmente la predicción de clase para los datos comparando los discriminantes, el error de predicción y la exactitud de la predicción total del modelo. =,=h3>>Resultados=,=p>>La hoja de trabajo donde se hizo la práctica puede descargarse en este enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD4",
        "descripcion":"En este ejercicio se usa una planilla electrónica para minimizar una función siguiendo los gradientes de la función de costo a partir de lo cual armar un modelo de regresión lineal.",
        "contenido":"h2>>Pasos=,=p>>- Se representó en la planilla un dataset con una variable de entrada X y una de salida Y.=,=p>> - Se graficaron los datos. =,=img>>UT3-PD4-1.jpg>>500>400=,=p>> - Se realizó el procedimiento de descenso de gradiente con alpha=0.01 en 24 iteraciones hallando valores para los coeficientes B0 y B1 que en una regresión lineal siguen la relación Y = B1xX + B0, la predicción para dicho modelo y el error de predicción. =,=p>> - Se graficó el error de predicción vs iteraciones. =,=img>>UT3-PD4-2.jpg>>500>400=,=p>> - Se calculó el error medio cuadrático RMSE. =,=p>> - Se generaron nuevos valores de entrada para X entre 0 y 8 con un paso 0.1 y se predijo su valor de Y a partir de un modelo de regresión lineal usando los coeficientes hallados al finalizar el procedimento mencionado anteriormente. El resultado fue graficado. =,=img>>UT3-PD4-3.jpg>>500>400 =,=p>> - Se analizaron los datos de entrada desde la óptica de los requerimientos para aplicar un método de regresión lineal.=,=br=,=h3>>Resultados=,=p>> La planilla de trabajo donde se realizaron los pasos descriptos anteriormente se encuentra accesible para la descarga con el enlace que se encuentra a continuación:=,=a>>Descargar plantilla>>Algoritmos lineales PD4.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD5",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>600>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100. =,=p>>- Se transformaron los atributos string a números. =,=p>> - Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equivalente en la herramienta RapidMiner.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>900>>300=,=br=,=a>>Descargar proceso>>Algoritmos Lineales PD5.rmp>>d=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD1",
        "descripcion":"Emplearemos un modelo CART para un problema de clasificación binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se ingresó el dataset de entrada y se graficaron los datos diferenciando las clases Y = 0 e Y = 1. =,=img>>UT4-PD1-1.jpg>>600>>200=,=p>> - Se utilizó la variable X1 para armar el modelo CART con un valor para el punto de división de X1 = 2.771244718 calculando el índice Gini del modelo producido. =,=p>> - Se repitió el proceso anterior con un punto de división de X1 = 6.642287351 y se obtuvo el valor del coeficiente Gini del CART generado además de una descripción del árbol de decisión generado.=,=h2>> Ejercicio 2 =,=p>> Se utilizó el CART producido al final del ejercicio anterior para hacer predicciones de un dataset de test y se calculó la exactitud.=,=h3>>Resultados=,=p>> Las planillas de trabajo en las que se hicieron los pasos explicados están accesibles para descargar con los siguientes enlaces:=,=a>>Descargar plantilla ejercicio 1>>Algoritmos no lineales PD1-1.xlsx>>d=,=br=,=a>>Descargar plantilla ejercicio 2>>Algoritmos no lineales PD1-2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD2",        
        "descripcion":"Estudiaremos la construcción de un modelo de árbol de decisión de regresión simple en la herramienta KNIME.",
        "contenido":"p>> Se descargó el tutorial de árbol de decisión simple KNIME de =,=a>>https://www.knime.com/nodeguide/analytics/regressions/learning-a-simple-regression-tree>>https://www.knime.com/nodeguide/analytics/regressions/learning-a-simple-regression-tree=,=br=,=img>>UT4-PD2-1.jpg>>800>500=,=h3>>Análisis=,=br=,=h5>>Descripción=,=p>> Este workflow utiliza el dataset Iris dividiéndolo con muestreo estratíficado en una parte de entrenamiento de un 80% y otra de prueba de un 20% para el caso de un modelo de árbol de regresión simple del cual se obtiene la performance y se la visualiza a través de una es gráfica. En este modelo a partir del ancho y el largo del sépalo, el largo de los pétalos y la clase de una flor entre Iris-setosa, Iris-versicolor y Iris virgínica predice el ancho de sus pétalos. =,=h5>> Componentes =,=h6>>File reader=,=p>>El operador equivalente a FileReader en RapidMiner es Retrieve. Estos se diferencian en que el primero deja editar características del dataset luego de ser incluido en el proyecto como los tipos para las variables y en RM esto se configura solo al importar el dataset. En el dataset sepal length, sepal width, petal length y pethal width son doubles y class es una string.=,=img>>UT4-PD2-2.jpg>>500>>650=,=br=,=br=,=h6>>Partitioning=,=p>> El operador partitioning ofrece elegir el tamaño de la partición y la forma en la que se quiere separar determinando por ejemplo si se eligen los primeros valores o si se hace muestreo estartíficado, da la opción para usar una seed como en RM, permite alternativas acerca de las Flow Variables que funcionan para a hacer variar ciertas configuraciones en el nodo de forma dinámica con cada ejecución y deja a elección políticas sobre el uso de la memoria. =,=p>> El operador equivalente a Patitioning en RapidMiner es Split. La principal diferencia es que mientras que en el componente de KNIME se hace una partición por cada unidad del componente, en RM se permite realizar varias a la vez.=,=img>>UT4-PD2-3.jpg>>500>>400=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> El proceso utiliza un algoritmo base de árbol de regresión simple siguiendo el algoritmo descripto en “Classification and Regression Tress” (Breiman et al, 1984) con algunas simplifaciones como no pruning, no necesariamente árboles binarios, tratar de encontrar la mejor dirección para los missing values, etc. En estos árboles de regesión el valor de predicción el valor para cada nodo hoja es la media de los registros dentro de ella y la predicción mejora cuanto menor sea la varianza de los valores dentro de una hoja. Por lo tanto, para armarlo en cada nodo se hacen splits que minimicen la suma de errores cuadráticos de los hijos. El operador Simple Regression Tree Learner soporta predictores de tipo numérico y categórico aunque solo soporta target columns de tipo numérico.=,=p>> Los parámetros que se pueden configurar del algoritmo son determinar el uso o no splits binarios para los atributos nominales, la forma en la que se manejan los missing values siendo XGBoost un algoritmo que calcula la mejor dirección para los valores faltantes y Surrogate que calcula para cada Split otros alternativos que mejoran la aproximación, el límite para la profundidad del árbol, el mínimo de valores que puede tener un nodo para que el Split se intente y el mínimo de registros que un nodo hijo puede tener.=,=img>>UT4-PD2-4.jpg>>700>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> Este operador recibe por un lado el modelo entrenado y por otro los datos de test, cómo salida tiene las predicciones realizadas. Permite modificar manualmente la columna de predicción, utlizar Flow Variables y decidir sobre políticas de memoria. =,=img>>UT4-PD2-5.jpg>>400>>300=,=br=,=br=,=h6>>Line Plot=,=p>> Este componente muestra una gráfica que compara los valores de predicción con la salida conocida para esos inputs del dataset de entrenamiento.=,=p>> Los parámetros que se pueden editar son el número de filas a mostrar, el límite del valor nominal a partir del cual una columna sea ignorada y la opción de colocar Flow Variables.=,=img>>UT4-PD2-6.jpg>>400>>300=,=img>>UT4-PD2-7.jpg>>500>>500=,=br=,=br=,=h6>>Numeric Scorer=,=p>> El operador funciona realizando los cálculos para los valores de coeficiente de terminación, media de error absoluto, error cuadrático medio y desviación media con signo de la predicción realizada.=,=img>>UT4-PD2-8.jpg>>500>>400=,=img>>UT4-PD2-9.jpg>>250>>125"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD3",        
        "descripcion":"En una hoja de cálculo analizaremos paso a paso el algoritmo de Support Vector Machines lineal con descenso de sub gradiente y lo utilizaremos para realizar predicciones.",
        "contenido":"h3>>Pasos=,=p>>- Se graficaron los datos en dos series para Y=1 y para Y=-1.=,=img>>UT4-PD3-1.jpg>>750>>250=,=br=,=br=,=p>>- Para hallar los coeficientes B1 y B2 del modelo SVM lineal B0 + B1 x X1 + B2 x X2 = 0 se utilizó el método del descenso de sub-gradiente. El coeficiente B0 fue descartado por lo cual la recta resultante pasa por el origen.=,=p>>Para aplicar este algoritmo se comienza con los coeficientes B1 y B2 en 0 y posteriormente se calcula un primer valor de salida con la fórmula dispuesta a continuación.=,=img>>UT4-PD3-2.jpg>>450>>50=,=p>>Si el valor de salida es mayor a 1 el patrón de entrenamiento no es un vector de soporte y por lo tanto se aplica la siguiente función para b1 y b2=,=img>>UT4-PD3-3.jpg>>250>>75=,=p>>En caso contrario se aplica la siguiente fórmula sobre los valores de los vectores utilizando en este caso lambda=0.45 y donde t es la iteración actual=,=img>>UT4-PD3-4.jpg>>550>>75=,=p>>- Este procedimiento fue realizado por 16 épocas en las que para cada una se itera sobre todo el dataset.=,=p>>- Se realizó una gráfica de la exactitud en función de las épocas obteniendo el siguiente resultado.=,=img>>UT4-PD3-5.jpg>>750>>350=,=br=,=br=,=p>>- Tras todas las iteraciones los coeficientes obtenidos fueron B1=0.55 Y B2=-0.72 obteniendo el plano 0.55xX1 -0.72xX2 = 0. Este se utilizó para calcular las predicciones con los datos de entrenamiento y se obtuvo un 100% de exactitud.=,=a>>Descargar planilla de trabajo>>Algoritmos no lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD4",
        "descripcion":"En este ejercicio se analizará el componente SVM de RapidMiner y se lo utilizará para resolver un problema no separable linealmente.",
        "contenido":"p>>El operador SVM utiliza la implementación de Java de support vector machine mySVM de Stefan Rueping pudiendo ser usado para regresión y clasificación. Este fue empleado para resolver un problema no separable linealmente primero con los parámetros por defecto y luego se lo resolvió modificando el kernel a polinomial.=,=p>> A continuación se describen y muestran los valores de los parámetros utilizados del operador y luego los resultados de performance obtenidos para ambos casos. =,=h5>>Parámetros=,=p>>- Kernel type: Tipo de función kernel a utilizar en el algoritmo. Valor inicial dot, final polynomial.=,=p>>- Kernel cache: Fija el tamaño de cache para las evaluaciones kernel. Valor: inicial 200, final 200.=,=p>>- C: Es una constante de complejidad que fija la tolerancia a clasificación errónea, cuando más alta más suaves son los límites y cuanto baja estos son más duros. Si es demasiado alta puede dar overfitting y si es muy baja puede dar over generalization. Valor: inicial 0.0, final 0.0.=,=p>>- Convergence épsilon: Especifica la precisión de las KKT conditions. Valor: inicial 0.01, final 0.01.=,=p>>- Max iterations: Número máximo de iteraciones. Valor: inicial 100000, final 100000.=,=p>>- Scale: Si está activado los valores se escalan. Valor: inicial activado, final activado.=,=p>>- Lpos: Factor para constante de complejidad del SVM caso positivos. Valor: inicial 1.0, final 1.0.=,=p>>- Lneg: Factor para constante de complejidad del SVM caso negativos. Valor: inicial 1.0, final 1.0.=,=p>>- Épsilon: Constante de insensibilidad. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon plus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon minus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Balance cost: Adapta Cpos y Cneg al tamaño relative de las clases. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss pos: Usa pérdida cuadrática para desviación positiva. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss neg: Usa pérdida cuadrática para desviación negativa. Valor: inicial desactivado, final desactivado.=,=br=,=h5>>Resultados=,=br=,=h6>>Caso inicial=,=img>>UT4-PD4-1.jpg>>900>>150=,=br=,=br=,=h6>>Caso final=,=img>>UT4-PD4-2.jpg>>900>>150=,=br=,=br=,=a>>Descargar proceso de RapidMiner>>UT4-TA7.rmp>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD5",
        "descripcion":"Comparación de árboles de decisión entre RapidMiner y Weka aplicados al dataset Iris. Además se analiza este componente en Azure ML Studio, KNIME y Python Sci kit learn.",
        "contenido":"p>>Se utiliza el dataset Iris y las herramientas RapidMiner y Weka para hacer una comparación de la performance de sus modelos de árboles de decisión midiéndola con un Split de datos de un 70% y un 30% para entrenamiento y test respectivamente en ambas.=,=h4>>RapidMiner=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión. =,=h6>>Parámetros=,=p>>- Criterion: gain_ratio. Selecciona el criterio que se usará para seleccionar los atributos sobre los que hacer los splits.=,=p>>- Maximal Depth: 10. La máxima profundidad del árbol.=,=p>>- Apply pruning: Activado. Si se aplica pruning o no.=,=p>>- Confidence: 0.1. Nivel de confianza usado para el error pesimista del cálculo de pruning.=,=p>>- Apply prepruning: Activado. Si se aplica prepruning o no=,=p>>- Minimal gain: 0.01. La ganancia de un nodo se calcula antes del Split. Se hace el Split si la ganancia es mayor al minimal gain.=,=p>>- Minimal leaf size: 2. Tamaño mínimo de observaciones por hoja.=,=p>>- Minimal size for split: 4. El tamaño de un nodo es el número de ejemplos en él. Solo se hace el Split para obtener nodos con un tamaño mayor al minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Numero de nodos alternativos testeados para un Split cuando el prepruning previene un Split.=,=br=,=h4>>Weka=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- batchSize: 100. Numero de instancias a procesar si la predicción batch está siendo realiza.=,=p>>- Debug: False. Si es verdadero el clasificador podría poner info adicional como salida en consola.=,=p>>- doNotCheckCapabilities: False. Si es verdadero las capacidades del clasificador no son checkeadas antes de la compilación=,=p>>- initialCount: 0.0. Valor inicial del contador de la clase.=,=p>>- MaxDepth: 10. Máxima profundidad del árbol, con -1 no hay restricción.=,=p>>- MinNum: 2.0. Mínimo peso total para las instancias de una hoja.=,=p>>- minVariancePrep: 0.001. Mínima proporción de la varianza de todos los datos que necesita estar en un nodo para que se haga un Split.=,=p>>- noPruning. False. Si se realiza pruning.=,=p>>- numDecimalPlaces. 2. Número de posiciones decimales para usar en la salida del modelo.=,=p>>- numFolds: 3. Determina el tamaño de los datos usados para pruning.=,=p>>- Seed. 1. La semilla usada para la aleatoriedad de los datos.=,=p>>- spreadIntialCount. False. Distribuir el recuento inicial en todos los valores en lugar de utilizar el recuento por valor.=,=br=,=h4>>Resultados=,=p>>Se obtuvo una performance de un 91.11% en el caso de RapidMiner y un 96% en Weka utilizando los parámetros que se muestran en la sección anterior para cada uno.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>1000>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>800>>800=,=br=,=br=,=h4>>Otras herramientas=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. Se utiliza una versión mejorada de los árboles de decisión con una gradient boosting machine.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- Maximum number of leaves per tree. Número máximo de hojas del árbol=,=p>>- Minimum number of samples per leaf node. Mínimo número de ejemplos en un nodo hoja.=,=p>>- Learning rate. Ritmo de aprendizaje.=,=p>>- Total number of trees constructed. Número de árboles construidos al entrenar el algoritmo.=,=p>>- Random number seed. Semilla de entrenamiento.=,=p>>- Allow unknown categorical levels. Seleccionado crea un nuevo nivel para cada atributo categórico.=,=br=,=h5>>KNIME=,=p>>Tipo de problema. Clasificación.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas o nominales. La variable objetivo solo puede ser nominal.=,=h6>>Parámetros=,=p>>- Class column. Selecciona la variable objetivo.=,=p>>- Quality measure. Para seleccionar la medida de calidad para la cual se calcularan los Splits. Las opciones son Gini index y Gain Ratio.=,=p>>- Pruning method. Pruning reduce el tamaño del árbol y evita el overfitting.=,=p>>- Reduced error pruning. Si se checkea se usa un simple método de pruning.=,=p>>- Min number records per node. Mínimo numero de registros requeridos por nodo.=,=p>>- Number records to store for view. Selecciona el número de registros guardados en un tree para la view.=,=p>>-  Average Split point. Al checkearla el valor para Split con atributos numéricos se determina según la media de los valores que separan a las dos particiones.=,=p>>- Number threads. Permite multiprocesamiento.=,=p>>- Skip nominal columns without domain information. Seleccionada las columnas nominales que no información de valores de dominio se saltean.=,=p>>- Force root split column. Seleccionada, el primer split es calculado en la columna elegida sin evaluar ninguna otra para posibles splits.=,=p>>- Binary nominal splits. Al seleccionarla a los atributos nominales se les hacen splits binarios.=,=p>>- Max nominal. Número máximo de valores nominales.=,=p>>- Filter invalid attribute values in child nodes. Habilitando esta opción se hace un post procesamiento del tree y se filtran checkeos inválidos.=,=p>>- No true child strategy: Opciones para cuando el valor de los atributos de un nodo es desconocido.=,=p>>- Missing value strategy. Opciones para los valores faltantes.=,=br=,=h5>>Python Sci kit learn=,=p>>Tipo de problema. Clasificación y regresión=,=p>>Algoritmo base. CART=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas. La variable objetivo puede ser nominal o numérica.=,=h6>>Parámetros=,=p>>- Criterio. Función para medir la calidad del Split. Puede ser Gini o entropy=,=p>>- Splitter. Estrategia utilizada para elegir el Split en cada nodo. Las opciones son mejor o random.=,=p>>- Max Depth. Máxima profundidad del árbol.=,=p>>- Min samples. Mínimo de ejemplos requeridos para hacer un Split generando un nodo interno.=,=p>>- Min samples leaf. Mínimo de ejemplos requeridos para hacer un Split generando un nodo hoja.=,=p>>- Min weight fraction. Fracción de peso mínimo del total de peso requerido en un nodo hoja.=,=p>>- Max features. Máximo número de variables de entrada considerado para hacer el mejor Split.=,=p>>- Random_state. Controla la aleatoriedad del estimador.=,=p>>- Max_leaf_node. Máximo número de nodos hoja.=,=p>>- Min_impurity_decrease: Se le hará un Split a un nodo si el Split da un decenso de la impureza mayor o igual a este valor.=,=p>>- Class_weight: Pesos asociados a las clases.=,=p>>- Ccp_alpha: Parámetro de complehidad usado para el pruning de mínimo costo-complejidad."
    },
    {
        "unidad":"Caso",
        "titulo":"Titanic",
        "descripcion":"Se realizará un análisis del problema y de los datos del dataset Titanic para a partir de este crear un modelo de regresión logística con la herramienta RapidMiner y otro equivalente con SciKitLearn, medir sus performances y compararlas.",
        "contenido":"h2>>Problema=,=p>>A partir de datos respecto al hundimiento de Titanic se pretende predecir si una persona con determinadas características sobrevivió o no por lo cual es un problema supervisado de clasificación binaria. Para este se propone una solución con un algoritmo lineal de regresión logística utilizando CrossValidation para el testeo de su performance.=,=h2>>Análisis de datos=,=p>>El dataset cuenta con 13 atributos de entrada más la variable objetivo \"survived\" para los cuales se cuenta con 1310 observaciones.=,=h4>>Atributos=,=p>>- pclass: Clase del pasajero. Tipo integer.=,=p>>- name. Nombre del pasajero. Tipo string.=,=p>>- sex. Sexo del viajante. Tipo string.=,=p>>- age. Edad de la persona. Tipo real. Hay 263 datos faltantes para esta variable.=,=p>>- sibsp: Número de hermanos a bordo. Tipo entero.=,=p>>- parch: Número de padres a bordo. Tipo entero.=,=p>>- ticket: Código del ticket del pasajero. Tipo string.=,=p>>- fare: Tarifa pagada. Tipo real. Hay un dato faltante para esta entrada.=,=p>>- cabin. Tipo string. Cabina del viajero. Tipo string. Tiene 1014 missing values. =,=p>>- embarked: Puerto en el que embarcó. Tipo string. Faltan dos de estos datos.=,=p>>- boat: Código del bote salvavida que usó la persona. Tipo string. Cuenta con 823 missing values.=,=p>>- body: Número de cuerpo si falleció. Tipo integer. Cuenta con 1188 missing values. =,=p>>- home.dest: Lugar de destino. Tipo string. Tiene 564 missing values.=,=p>>- survived: Si sobrevivió o no. Tipo integer.=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>Entre las entradas hay variables con alto porcentaje de datos faltantes por lo que no se contó con ellas siendo estas body con un 90.7% y cabin con un 77.4%, para la variable age se reemplazaron los valores faltantes por el promedio, se eliminaron las filas con missing values para fare y embarked y se le asignó una categoría única para los missing values de boat y home.dest. Luego de todo este proceso se eliminaron todos los missing values.=,=h5>>Outliers=,=p>>El algoritmo de regresión logística es altamente influenciable a los outliers y por lo tanto tras observar los histogramas de los datos para los distintos parámetros, que se encuentran a continuación, se eliminaron aquellos con una alguna/s de las siguientes condiciones age>=70, sibsp>=6, parch>=6 o fare>=350.=,=img>>caso-2-2.jpg>>300>>150=,=img>>caso-2-3.jpg>>300>>150=,=br=,=img>>caso-2-4.jpg>>300>>150=,=img>>caso-2-5.jpg>>300>>150=,=h5>>Correlación=,=p>>Observando la matriz de correlación se observó que hay una correlación grande entre home.dest y pclass con 0.887 y otra bastante significativa con un 0.612 entre fare y pclass de forma inversamente proporcional. Por lo tanto se decidió quitar pclass.=,=img>>caso-2-6.jpg>>600>>200=,=br=,=br=,=h5>>Selección de atributos=,=p>>Se descartaron las variables name y ticket por ser ids.=,=h3>>Modelo=,=p>>Se realizó el procedimiento de data pre-processing descripto anteriormente y luego se utilizó un algoritmo de regresión logística con CrossValidation de 10 folds y muestreo estratificado para el testeo.=,=h5>>Diseño de RapidMiner=,=img>>caso-2-7.jpg>>800>>400=,=p>>A continuación se mostrará la configuración bloque a bloque del esquema de RapidMiner y se presentará a la vez el código del modelo idéntico creado en SciKitLearn.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>400>>75.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>600>>400.=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\",\"name\",\"pclass\",\"ticket\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>Primer replace Missing Values=,=img>>caso-2-rm-3.jpg>>400>>175=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=br=,=h5>>Segundo replace Missing Values=,=img>>caso-2-rm-4.jpg>>600>>400=,=br=,=code>>.dataset[\"boat\"].fillna('0', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('0', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>600>>400.=,=br=,=code>>.dataset = dataset[dataset['sibsp'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['parch'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['age'] <= 70]=,=br=,=code>>.dataset = dataset[dataset['fare'] <= 350]=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6.jpg>>600>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\"sex\"])=,=br=,=code>>.encoded_sex_training = le.transform(dataset[\"sex\"])=,=br=,=code>>.dataset[\"sex\"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\"embarked\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"embarked\"])=,=br=,=code>>.dataset[\"embarked\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"home.dest\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"home.dest\"])=,=br=,=code>>.dataset[\"home.dest\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"boat\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"boat\"])=,=br=,=code>>.dataset[\"boat\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2-rm-7.jpg>>1000>>150.=,=br=,=code>>.X = dataset.drop(labels=[\"survived\"], axis=1)=,=br=,=code>>.y = dataset[\"survived\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>>.cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)=,=br=,=code>>.print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))=,=br=,=br=,=h4>>Resultados=,=p>>Se obtuvo un muy alto nivel de presición en RapidMiner con un 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>1000>>200=,=img>>caso-2-10.jpg>>200>>25=,=p>> En cuanto al modelo de SciKitLearn se obtuvo una presición de 94.5% +/- 1.2%. De esta forma se puede destacar que se obtuvieron presiciones similares para de los modelos idénticos de regresión logística aplicados al dataset Titanic realizados en las dos herramientas.=,=br=,=a>>Descargar modelo RapidMiner>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Titanic Logistic Regression.py>>d"
    },
    {
        "unidad":"Caso",
        "titulo":"Deportes",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h3>>Problema=,=p>>Según características personales se desea predecir cual es el mejor deporte para una persona por lo cual este es un problema supervisado de clasificación. Para este se planteará un modelado con Linear Descriminant Analysis a continuación.=,=h3>>Análisis de datos=,=p>>El dataset tiene 8 columnas de entrada numéricas y la variable objetivo categórica \"DeportePrimario\" conteniendo 493 filas.=,=h4>>Atributos=,=p>>- Edad=,=p>>- Fuerza=,=p>>- Velocidad =,=p>>- Lesiones=,=p>>- Vision =,=p>>- Resistencia =,=p>>- Agilidad =,=p>>- CapacidadDecision =,=p>>- DeportePrimario=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>En el dataset no hay datos faltantes.=,=h5>>Outliers=,=p>>Para LDA es importante quitar los outliers y tras observar los histogramas de los atributos se decidió quitar los datos con CapacidadDecision<3 o CapacidadDecision>100. La distribución para esta variable es la siguiente.=,=img>>caso-3-1.jpg>>300>>200=,=h4>>Estándarización=,=p>>El algoritmo a utilizar asume que las variables de entrada tienen la misma varianza por lo que es buena idea estandarizar los datos para que tengan media 0 y varianza 1.=,=h3>>Modelo=,=p>>El procedimiento descripto anteriormente y el modelo LDA fueron realizados de forma equivalente en RapidMiner y SciKitLearn. Se contó con un dataset de entrenamiento y uno de prueba para el cual realizar predicciones.=,=h4>>Diseño de RapidMiner=,=img>>UT3-PD5-3.jpg>>900>>300=,=br=,=h4>>Código SciKitLearn=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar proceso>>Sports LDA.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Sports LDA.py>>d=,=br=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    }
]