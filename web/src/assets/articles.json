[
    {
        "unidad":"CRISP-DM",
        "titulo":"PD1",
        "descripcion":"En la siguiente práctica se realizaron tutoriales de RapidMiner para preparación de datos.",
        "contenido":"h3>> Tutorial Handling Missing Values =,=img>>UT2-PD1-1.jpg>>1000>>200 =,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>1200>>200=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d"
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"PD2",
        "descripcion":"En este ejercicio se hicieron tutoriales de construcción de modelos para la herramienta RapidMiner.",
        "contenido":"h3>> Modeling =,=img>>UT2-PD2-1.jpg>>800>>200 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>800>>200 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>1100>>200 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>900>>200=,=img>>UT2-PD2-4-2.jpg>>1200>>200 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>1000>>150=,=img>>UT2-PD2-5-2.jpg>>1000>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d"
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"PD3",
        "descripcion":"En este ejercicio se aplican técnicas de pre procesamiento y obtención de estadísticas para el dataset Wine utiliando funciones de Python.",
        "contenido":"h3>>Pasos=,=p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0    dataset = list()=,=br=,=code>>.\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0    minmax = list()=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0    train = list()=,=br=,=code>>.\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.\u00a0for i in range(10):    =,=br=,=code>>.\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>1400>>400=,=img>>UT2-PD3-2.jpg>>1400>>400=,=img>>UT2-PD3-3.jpg>>1400>>400=,=img>>UT2-PD3-4.jpg>>1400>>    800=,=img>>UT2-PD3-5.jpg>>1400>>800"
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"PD4",
        "descripcion":"Construcción de un modelo de predicción para el dataset Titnic y de un programa que responda a consultas sobre el dataset mencionado.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=br=,=p>> - Se importaron librerías y paquetes.=,=p>> - Se cargó el dataset y se los mostró.=,=p>> - Se gestionaron los datos faltantes.=,=p>> - Se graficaron los datos según Age vs Survived y Fare vs Survived..=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\"-----DATASET------\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----MISSSING VALUES------\")=,=br=,=code>>.print(\"*Antes de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Se reemplazan los missing values de edad por la media=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan las columnas body y cabin por altos porcentajes =,=br=,=code>>.# de missing values siendo estos 90.7% y 77% respectivamente=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan la fila sin valor para fare y las dos filas=,=br=,=code>>.# Ssin valor para embarked=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#Se asigna una categoría única para los missing values de boay y home.dest=,=br=,=code>>.dataset[\"boat\"].fillna('U', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('U', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"*Despues de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----GRAFICAS------\")=,=br=,=code>>.print(\"- x = Age, y = Survived\")=,=br=,=code>>.print(\"- x = Fare, y = Survived\")=,=br=,=code>>.colors = (\"red\", \"blue\")=,=br=,=code>>.plt.scatter(dataset['age'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset['fare'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-1.jpg>>300>>600=,=img>>UT2-PD4-2.jpg>>400>>300=,=img>>UT2-PD4-3.jpg>>400>>300   =,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>> - Se calculó la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> - Se calculó la probabilidad de que un niño de 10 años o menos de 3ra clase sobreviva. =,=br=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilidades:\")=,=br=,=code>>.print(\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3)))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>>.print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>500>>200"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD1",
        "descripcion":"Utilizando una planilla electrónica se generó un modelo de regresión linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se hizo uso de los modelos para hacer predicciones.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se calcularon los coeficientes para un modelo de regresión lineal simple Y = B0 + B1 x X. Para ello se utilizarán las fórmulas:=,=img>>UT3-PD1-1.jpg>>600>>100=,=img>>UT3-PD1-2.jpg>>500>>50=,=p>> -Se aplicó el modelo a los valores de entrenamiento.=,=p>> -Se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gráfico para mostrar la recta de ajuste.=,=p>> -Se estimó el error de predicción RMSE con esta ecuación:=,=img>>UT3-PD1-3.jpg>>400>>100=,=p>> -Se utilizó un método alternativo para calcular B1 y a partir de este B0 al igual que antes. La fórmula utilizada fue la que se encuentra a continuación:=,=img>>UT3-PD1-4.jpg>>400>250=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>>- Se estimaron los coeficientes del modelo simple de regresión lineal Y = B0 + B1 x X realizando 24 iteraciones.=,=p>> -Se realizó el gráfico error de predicción contra iteraciones =,=p>> -Se calculó el error medio cuadrático RMSE.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD2",
        "descripcion":"En una planilla electrónica se visualizará una función logística y se genararán modelos de regresión logística con descenso de gradiente estocástico.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se crearon 10 valores para una variable X con distribución normal=,=p>> -Se aplicó la función logística a los anteriores con coeficientes elegidos arbitrariamente para hallar valores de una variable Y.=,=p>> -Se generó un gráfico para visualizar la función.=,=img>>UT3-PD2-1.jpg>>700>>300=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>> - A partir de un conjunto de datos se hizo una gráfica con dos series, una para Y=0 y otra para Y=1. =,=p>> - Se estimaron los coeficientes del modelo de regresión logística Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estocástico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 épocas. =,=p>> - Se calculó la predicción, el cuadrado del error, el error de clase para cada ejemplo de entrenamiento de cada época.=,=p>> - Se calculó la exactitud y el error medio cuadrático RMSE para cada época.=,=p>> - Se realizaron las gráficas exactitud vs época y RMSE vs época.=,=p>> - Se repitió todo el proceso anterior para un nuevo conjunto de datos.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD3",
        "descripcion":"Empleando una planilla electrónica se generará un modelo de análisis de discriminante lineal LDA y con él se hará la predicción de clase para un conjunto de datos con un atributo y una salida con dos clases.",
        "contenido":"h3>>Pasos=,=p>> - Se insertaron los valores del dataset. =,=p>> - Se graficaron los datos separando según las clases de salida. =,=p>> - Se calcularon, siendo n el número de clases, las medias para cada clase k y la varianza de x con las ecuaciones que se encuentran a continuación.=,=img>>UT3-PD3-1.jpg>>250>>75=,=img>>UT3-PD3-2.jpg>>350>>75 =,=p>> - Se calculó la predicción de clases usando: =,=img>>UT3-PD3-4.jpg>>400>200 =,=p>> - Se utilizó la siguiente ecuación para calcular los discriminantes de x para ambas clases Y = 0 e Y = 1.=,=img>>UT3-PD3-3.jpg>>500>>60=,=p>> -Se calculó finalmente la predicción de clase para los datos comparando los discriminantes, el error de predicción y la exactitud de la predicción total del modelo. =,=h3>>Resultados=,=p>>La hoja de trabajo donde se hizo la práctica puede descargarse en este enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD4",
        "descripcion":"En este ejercicio se usa una planilla electrónica para minimizar una función siguiendo los gradientes de la función de costo a partir de lo cual armar un modelo de regresión lineal.",
        "contenido":"h2>>Pasos=,=p>>- Se representó en la planilla un dataset con una variable de entrada X y una de salida Y.=,=p>> - Se graficaron los datos. =,=img>>UT3-PD4-1.jpg>>500>400=,=p>> - Se realizó el procedimiento de descenso de gradiente con alpha=0.01 en 24 iteraciones hallando valores para los coeficientes B0 y B1 que en una regresión lineal siguen la relación Y = B1xX + B0, la predicción para dicho modelo y el error de predicción. =,=p>> - Se graficó el error de predicción vs iteraciones. =,=img>>UT3-PD4-2.jpg>>500>400=,=p>> - Se calculó el error medio cuadrático RMSE. =,=p>> - Se generaron nuevos valores de entrada para X entre 0 y 8 con un paso 0.1 y se predijo su valor de Y a partir de un modelo de regresión lineal usando los coeficientes hallados al finalizar el procedimento mencionado anteriormente. El resultado fue graficado. =,=img>>UT3-PD4-3.jpg>>500>400 =,=p>> - Se analizaron los datos de entrada desde la óptica de los requerimientos para aplicar un método de regresión lineal.=,=br=,=h3>>Resultados=,=p>> La planilla de trabajo donde se realizaron los pasos descriptos anteriormente se encuentra accesible para la descarga con el enlace que se encuentra a continuación:=,=a>>Descargar plantilla>>Algoritmos lineales PD4.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD5",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python para realizar la clasificación a partir de conjuntos de datos.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión.=,=br=,=h3>>Código=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.linear_model import LogisticRegression=,=br=,=code>>.from sklearn.metrics import confusion_matrix, classification_report=,=br=,=code>>.from sklearn.model_selection import train_test_split=,=br=,=code>>.from sklearn import preprocessing=,=br=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>600>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100. =,=p>>- Se transformaron los atributos string a números. =,=p>> - Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equilvalente en la herramienta RapidMiner.=,=h3>>Código=,=code>> .import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.linear_model import LogisticRegression=,=br=,=code>>.from sklearn.metrics import confusion_matrix, classification_report=,=br=,=code>>.from sklearn.model_selection import train_test_split=,=br=,=code>>.from sklearn import preprocessing=,=br=,=code>>.=,=br=,=code>>.input_file = \"../sports_Training.csv\"=,=br=,=code>>.data_original = pd.read_csv(input_file, header=0)=,=br=,=code>>.#print(data_original.values)=,=br=,=code>>.=,=br=,=code>>.data = data_original[(data_original['CapacidadDecision'] >= 3) &(data_original['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.X = data[['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']].values=,=br=,=code>>.Y = data['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.#le = preprocessing.LabelEncoder()=,=br=,=code>>.#Y_encoded = le.fit_transform(data['DeportePrimario'].values)=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.input_file2 = \"../sports_Scoring.csv\"=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data = data_scoring[(data_scoring['CapacidadDecision'] >= 3) &(data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.Xsco = data[['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']].values=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>1200>>400=,=br=,=a>>Descargar proceso>>Algoritmos Lineales PD5.rmp>>d=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 97.85% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD1",
        "descripcion":"Emplearemos un modelo CART para un problema de clasificación binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se ingresó el dataset de entrada y se graficaron los datos diferenciando las clases Y = 0 e Y = 1. =,=img>>UT4-PD1-1.jpg>>600>>200=,=p>> - Se utilizó la variable X1 para armar el modelo CART con un valor para el punto de división de X1 = 2.771244718 calculando el índice Gini del modelo producido. =,=p>> - Se repitió el proceso anterior con un punto de división de X1 = 6.642287351 y se obtuvo el valor del coeficiente Gini del CART generado además de una descripción del árbol de decisión generado.=,=h2>> Ejercicio 2 =,=p>> Se utilizó el CART producido al final del ejercicio anterior para hacer predicciones de un dataset de test y se calculó la exactitud.=,=h3>>Resultados=,=p>> Las planillas de trabajo en las que se hicieron los pasos explicados están accesibles para descargar con los siguientes enlaces:=,=a>>Descargar plantilla ejercicio 1>>Algoritmos no lineales PD1-1.xlsx>>d=,=br=,=a>>Descargar plantilla ejercicio 2>>Algoritmos no lineales PD1-2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"PD2",
        "descripcion":"Estudiaremos un modelo de árbol de decisión de regresión simple en KNIME.",
        "contenido":"p>> Se descargó el tutorial de árbol de decisión simple KNIME de =,=a>>https://www.knime.com/nodeguide/analytics/regressions/learning-a-simple-regression-tree>>https://www.knime.com/nodeguide/analytics/regressions/learning-a-simple-regression-tree=,=br=,=img>>UT4-PD2-1.jpg>>800>500=,=h3>>Análisis=,=br=,=h5>>Descripción=,=p>> Este workflow utiliza el dataset Iris dividiéndolo con muestreo estratíficado en una parte de entrenamiento de un 80% y otra de prueba de un 20% para el caso de un modelo de árbol de regresión simple del cual se obtiene la performance y se la visualiza a través de una es gráfica. En este modelo a partir del ancho y el largo del sépalo, el largo de los pétalos y la clase de una flor entre Iris-setosa, Iris-versicolor y Iris virgínica predice el ancho de sus pétalos. =,=h5>> Componentes =,=h6>>File reader=,=p>>El operador equivalente a FileReader en RapidMiner es Retrieve. Estos se diferencian en que el primero deja editar características del dataset luego de ser incluido en el proyecto como los tipos para las variables y en RM esto se configura solo al importar el dataset. En el dataset sepal length, sepal width, petal length y pethal width son doubles y class es una string.=,=img>>UT4-PD2-2.jpg>>500>>650=,=br=,=br=,=h6>>Partitioning=,=p>> El operador partitioning ofrece elegir el tamaño de la partición y la forma en la que se quiere separar determinando por ejemplo si se eligen los primeros valores o si se hace muestreo estartíficado, da la opción para usar una seed como en RM, permite alternativas acerca de las Flow Variables que funcionan para a hacer variar ciertas configuraciones en el nodo de forma dinámica con cada ejecución y deja a elección políticas sobre el uso de la memoria. =,=p>> El operador equivalente a Patitioning en RapidMiner es Split. La principal diferencia es que mientras que en el componente de KNIME se hace una partición por cada unidad del componente, en RM se permite realizar varias a la vez.=,=img>>UT4-PD2-3.jpg>>500>>400=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> El proceso utiliza un algoritmo base de árbol de regresión simple siguiendo el algoritmo descripto en “Classification and Regression Tress” (Breiman et al, 1984) con algunas simplifaciones como no pruning, no necesariamente árboles binarios, tratar de encontrar la mejor dirección para los missing values, etc. En estos árboles de regesión el valor de predicción el valor para cada nodo hoja es la media de los registros dentro de ella y la predicción mejora cuanto menor sea la varianza de los valores dentro de una hoja. Por lo tanto, para armarlo en cada nodo se hacen splits que minimicen la suma de errores cuadráticos de los hijos. El operador Simple Regression Tree Learner soporta predictores de tipo numérico y categórico aunque solo soporta target columns de tipo numérico.=,=p>> Los parámetros que se pueden configurar del algoritmo son determinar el uso o no splits binarios para los atributos nominales, la forma en la que se manejan los missing values siendo XGBoost un algoritmo que calcula la mejor dirección para los valores faltantes y Surrogate que calcula para cada Split otros alternativos que mejoran la aproximación, el límite para la profundidad del árbol, el mínimo de valores que puede tener un nodo para que el Split se intente y el mínimo de registros que un nodo hijo puede tener.=,=img>>UT4-PD2-4.jpg>>700>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> Este operador recibe por un lado el modelo entrenado y por otro los datos de test, cómo salida tiene las predicciones realizadas. Permite modificar manualmente la columna de predicción, utlizar Flow Variables y decidir sobre políticas de memoria. =,=img>>UT4-PD2-5.jpg>>400>>300=,=br=,=br=,=h6>>Line Plot=,=p>> Este componente muestra una gráfica que compara los valores de predicción con la salida conocida para esos inputs del dataset de entrenamiento.=,=p>> Los parámetros que se pueden editar son el número de filas a mostrar, el límite del valor nominal a partir del cual una columna sea ignorada y la opción de colocar Flow Variables.=,=img>>UT4-PD2-6.jpg>>400>>300=,=img>>UT4-PD2-7.jpg>>500>>500=,=br=,=br=,=h6>>Numeric Scorer=,=p>> El operador funciona realizando los cálculos para los valores de coeficiente de terminación, media de error absoluto, error cuadrático medio y desviación media con signo de la predicción realizada.=,=img>>UT4-PD2-8.jpg>>500>>400=,=img>>UT4-PD2-9.jpg>>500>>250"
    },
    {
        "unidad":"Casos",
        "titulo":"Predicción de altura según restos óseos",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> chau "
    }
]