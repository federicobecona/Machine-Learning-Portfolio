[
    {
        "unidad":"CRISP-DM",
        "titulo":"PD3",
        "descripcion":"En este ejercicio se aplican técnicas de pre procesamiento y obtención de estadísticas para el dataset Wine utiliando funciones de Python.",
        "contenido":"h3>>Pasos=,=p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    dataset = list()=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    minmax = list()=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    train = list()=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas de los dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas de los dataset de test:\")=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0for i in range(10):    =,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>1400>>400=,=img>>UT2-PD3-2.jpg>>1400>>400=,=img>>UT2-PD3-3.jpg>>1400>>400=,=img>>UT2-PD3-4.jpg>>1400>>    800=,=img>>UT2-PD3-5.jpg>>1400>>800"
    },
    {
        "unidad":"CRISP-DM",
        "titulo":"PD4",
        "descripcion":"Construiremos un modelo de predicción para el dataset Titnic y haremos un programa que responda a consultas sobre el dataset mencionado.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=br=,=p>> -Se importaron librerías y paquetes.=,=p>> -Se cargó el dataset y se los mostró.=,=p>> -Se gestionaron los datos faltantes.=,=p>> -Se graficaron los datos.=,=p>> -Se realizó Feature Engineering.=,=h3>>Código=,=h1>>!!!!!!!!!!!!!!!!!!!!!!RELLENAR!!!!!!!!!!!!!!!!!!!!!!!=,=code>>=,=br=,=br=,=h3>>Resultados=,=h1>>!!!!!!!!!!!!!!!!!!!!!!RELLENAR!!!!!!!!!!!!!!!!!!!!!!!=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>> -Se calculó la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> -Se calculó la probabilidad de que un niño de 10 años o menos de 3ra clase sobreviva. =,=br=,=h3>>Código=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilidades:\")=,=br=,=code>>.print(\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3)))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>>.print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>500>>200"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD1",
        "descripcion":"Utilizando una planilla electrónica se generó un modelo de regresión linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se hizo uso de los modelos para hacer predicciones.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se calcularon los coeficientes para un modelo de regresión lineal simple Y = B0 + B1 x X. Para ello se utilizarán las fórmulas:=,=img>>UT3-PD1-1.jpg>>600>>100=,=img>>UT3-PD1-2.jpg>>500>>50=,=p>> -Se aplicó el modelo a los valores de entrenamiento.=,=p>> -Se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gráfico para mostrar la recta de ajuste.=,=p>> -Se estimó el error de predicción RMSE con esta ecuación:=,=img>>UT3-PD1-3.jpg>>400>>100=,=p>> -Se utilizó un método alternativo para calcular B1 y a partir de este B0 al igual que antes. La fórmula utilizada fue la que se encuentra a continuación:=,=img>>UT3-PD1-4.jpg>>400>250=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>>- Se estimaron los coeficientes del modelo simple de regresión lineal Y = B0 + B1 x X realizando 24 iteraciones.=,=p>> -Se realizó el gráfico error de predicción contra iteraciones =,=p>> -Se calculó el error medio cuadrático RMSE.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD2",
        "descripcion":"En una planilla electrónica se visualizará una función logística y se genararán modelos de regresión logística con descenso de gradiente estocástico.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>> -Se generaron 10 valores para una variable X con distribución normal=,=p>> -Se aplicó la función logística a los anteriores con coeficientes elegidos arbitrariamente para hallar valores de una variable Y.=,=p>> -Se generó un gráfico para visualizar la función.=,=img>>UT3-PD2-1.jpg>>700>>300=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=br=,=p>> - A partir de un conjunto de datos se hizo una gráfica con dos series, una para Y=0 y otra para Y=1. =,=p>> - Se estimaron los coeficientes del modelo de regresión logística Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estocástico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 épocas. =,=p>> - Se calculó la predicción, el cuadrado del error, el error de clase para cada ejemplo de entrenamiento de cada época.=,=p>> - Se calculó la exactitud y el error medio cuadrático RMSE para cada época.=,=p>> - Se realizaron las gráficas exactitud vs época y RMSE vs época.=,=p>> - Se repitió todo el proceso anterior para un nuevo conjunto de datos.=,=h3>>Resultados=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"PD5",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python para realizar la clasificación a partir de conjuntos de datos.",
        "contenido":"h2>>Ejercicio 1=,=br=,=h3>>Pasos=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión.=,=br=,=h3>>Código=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.linear_model import LogisticRegression=,=br=,=code>>.from sklearn.metrics import confusion_matrix, classification_report=,=br=,=code>>.from sklearn.model_selection import train_test_split=,=br=,=code>>.from sklearn import preprocessing=,=br=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>600>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=br=,=h3>>Pasos=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100=,=p>>- Se transformaron los atributos string a números=,=p>>- Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equilvalente en la herramienta RapidMiner.=,=h3>>Código=,=code>> .import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.linear_model import LogisticRegression=,=br=,=code>>.from sklearn.metrics import confusion_matrix, classification_report=,=br=,=code>>.from sklearn.model_selection import train_test_split=,=br=,=code>>.from sklearn import preprocessing=,=br=,=code>>.=,=br=,=code>>.input_file = \"../sports_Training.csv\"=,=br=,=code>>.data_original = pd.read_csv(input_file, header=0)=,=br=,=code>>.#print(data_original.values)=,=br=,=code>>.=,=br=,=code>>.data = data_original[(data_original['CapacidadDecision'] >= 3) &(data_original['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.X = data[['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']].values=,=br=,=code>>.Y = data['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.#le = preprocessing.LabelEncoder()=,=br=,=code>>.#Y_encoded = le.fit_transform(data['DeportePrimario'].values)=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.input_file2 = \"../sports_Scoring.csv\"=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data = data_scoring[(data_scoring['CapacidadDecision'] >= 3) &(data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.Xsco = data[['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']].values=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>1200>>400=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 91.28% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Casos",
        "titulo":"Predicción de valores inmobiliarios",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> HELLO"
    },
    {
        "unidad":"Casos",
        "titulo":"Predicción de altura según restos óseos",
        "descripcion":"Esta TA consiste en...",
        "contenido":"h1>>Visitar esta página =,=p>> chau "
    }
]